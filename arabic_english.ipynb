{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Required Libraries and package of python\n",
    "import nltk\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import KeyedVectors\n",
    "from nltk.tokenize import sent_tokenize,word_tokenize\n",
    "import numpy as np\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.stats import spearmanr\n",
    "from scipy import spatial\n",
    "import spacy\n",
    "nlp=spacy.load('en_core_web_sm')\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "import nltk \n",
    "from nltk.tokenize import LineTokenizer \n",
    "from nltk.stem import PorterStemmer     \n",
    "tk = LineTokenizer()   \n",
    "lm = WordNetLemmatizer() \n",
    "from nltk.corpus import wordnet as wn\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pretrained Word2Vec model. It was pretrained on Google News Corpus\n",
    "filename = 'GoogleNews-vectors-negative300.bin'\n",
    "model = KeyedVectors.load_word2vec_format(filename, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "sw=stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250\n"
     ]
    }
   ],
   "source": [
    "#STS-2017 Dataset is used here.Here, we are taking the first sentence and second sentence into two list. \n",
    "fh=open('arabic_english_data.txt')\n",
    "c=0\n",
    "s1=[]\n",
    "s2=[]\n",
    "for line in fh:\n",
    "    line=line.lower()\n",
    "    c+=1\n",
    "    line=line.rstrip()\n",
    "    #print(c)\n",
    "    sentences=sent_text = sent_tokenize(line)\n",
    "    #print(c, sentences)\n",
    "    s1.append(sentences[0])\n",
    "    s2.append(sentences[1]) \n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length: 250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-33-1e2ce0dbec48>:25: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  if word in model.wv.vocab:\n",
      "<ipython-input-33-1e2ce0dbec48>:28: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  if w in model.wv.vocab:\n",
      "<ipython-input-33-1e2ce0dbec48>:35: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  if word in model.wv.vocab:\n",
      "<ipython-input-33-1e2ce0dbec48>:38: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  if w in model.wv.vocab:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson: 66.45113259682897 Spearman: 68.84110793004312\n"
     ]
    }
   ],
   "source": [
    "file=open('resultwordcommean1ar.txt','w')\n",
    "ns1=[]\n",
    "ns2=[]\n",
    "i=0\n",
    "print('Length:',len(s1))\n",
    "while(i<len(s1)):\n",
    "    ns1=s1[i].split(' ')\n",
    "    s1word=[]\n",
    "    for word in ns1:\n",
    "        for c in string.punctuation:\n",
    "            word=word.replace(c,'')\n",
    "        if word not in sw:\n",
    "            s1word.append(lemmatizer.lemmatize(word))\n",
    "    #print(s1word)\n",
    "    s2word=[]\n",
    "    ns2=s2[i].split(' ')\n",
    "    for word in ns2:\n",
    "        for c in string.punctuation:\n",
    "                word=word.replace(c,\"\")\n",
    "        if word not in sw:\n",
    "            s2word.append(lemmatizer.lemmatize(word))\n",
    "    #print(s2word)\n",
    "    max_score=[]\n",
    "    for word in s1word:\n",
    "        if word in model.wv.vocab:\n",
    "            score=[]\n",
    "            for w in s2word:\n",
    "                if w in model.wv.vocab:\n",
    "                    score.append(model.similarity(word,w))\n",
    "        max_score.append(max(score))\n",
    "    #print(max_score)\n",
    "    \n",
    "    max_score2=[]\n",
    "    for word in s2word:\n",
    "        if word in model.wv.vocab:\n",
    "            score=[]\n",
    "            for w in s1word:\n",
    "                if w in model.wv.vocab:\n",
    "                    score.append(model.similarity(word,w))\n",
    "        max_score2.append(max(score))\n",
    "    #print(max_score2)\n",
    "    \n",
    "    count=0\n",
    "    summ=0\n",
    "    for val in max_score:\n",
    "        summ+=val\n",
    "        count+=1\n",
    "    average=(summ*5)/count\n",
    "    \n",
    "    count1=0\n",
    "    summ1=0\n",
    "    for val in max_score2:\n",
    "        summ1+=val\n",
    "        count1+=1\n",
    "    average1=(summ1*5)/count1\n",
    "    \n",
    "    aver=(average+average1)/2\n",
    "    #print('i:',i+1,aver)\n",
    "    a=str(round(aver,6))+'\\n'\n",
    "    file.writelines(a)\n",
    "    i+=1\n",
    "    \n",
    "file.close()\n",
    "data1=[]\n",
    "data2=[]\n",
    "f1=open('resultwordcommean1ar.txt')\n",
    "f2=open('STS.gs.track2.ar-en.txt')\n",
    "def listvalue(file):\n",
    "    value=[]\n",
    "    for line in file:\n",
    "        value.append(float(line)/5)\n",
    "    return value\n",
    "data1=listvalue(f1)\n",
    "data2=listvalue(f2)\n",
    "#print(len(data1),len(data2))\n",
    "#mse = np.square(np.subtract(data2,data1)).mean()\n",
    "#print('MSE:',mse)\n",
    "corr,_=pearsonr(data1,data2)\n",
    "scor,_=spearmanr(data1,data2)\n",
    "print('Pearson:',corr*100,'Spearman:',scor*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length: 250\n",
      "Pearson: 61.85637989337756 Spearman: 65.5204236906536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-34-58a3a58ad988>:14: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  if(word in model.wv.vocab):\n",
      "<ipython-input-34-58a3a58ad988>:24: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  if(word in model.wv.vocab):\n"
     ]
    }
   ],
   "source": [
    "file=open('result1ar.txt','w')\n",
    "ns1=[]\n",
    "ns2=[]\n",
    "i=0\n",
    "print('Length:',len(s1))\n",
    "while(i<len(s1)):\n",
    "    ns1=s1[i].split(' ')\n",
    "    sum1=np.zeros((300,), dtype=\"float32\")\n",
    "    for word in ns1:\n",
    "        for c in string.punctuation:\n",
    "                word=word.replace(c,\"\")\n",
    "        #print(word)\n",
    "        if word not in sw:\n",
    "            if(word in model.wv.vocab): \n",
    "                sum1=np.add(sum1,model[word])\n",
    "    average1=np.divide(sum1,len(ns1),dtype=\"float32\")\n",
    "            \n",
    "    ns2=s2[i].split(' ')\n",
    "    sum2=np.zeros((300,),dtype=\"float32\")\n",
    "    for word in ns2:\n",
    "        for c in string.punctuation:\n",
    "                word=word.replace(c,\"\")\n",
    "        if word not in sw:\n",
    "            if(word in model.wv.vocab):\n",
    "                sum2=np.add(sum2,model[word])\n",
    "    average2=np.divide(sum2,len(ns2),dtype=\"float32\")\n",
    "    sim = 1 - spatial.distance.cosine(average1,average2)\n",
    "    sim=sim*5\n",
    "    a=str(round(sim,6))+'\\n'\n",
    "    file.writelines(a)\n",
    "    #print(i,'similarity',sim)\n",
    "    i+=1 \n",
    "file.close()\n",
    "\n",
    "\n",
    "data1=[]\n",
    "data2=[]\n",
    "f1=open('result1ar.txt')\n",
    "f2=open('STS.gs.track2.ar-en.txt')\n",
    "def listvalue(file):\n",
    "    value=[]\n",
    "    for line in file:\n",
    "        value.append(float(line)/5)\n",
    "    return value\n",
    "data1=listvalue(f1)\n",
    "data2=listvalue(f2)\n",
    "corr,_=pearsonr(data1,data2)\n",
    "scor,_=spearmanr(data1,data2)\n",
    "print('Pearson:',corr*100,'Spearman:',scor*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length: 250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-35-9fae6c049214>:40: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  if word in model.wv.vocab:\n",
      "<ipython-input-35-9fae6c049214>:42: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  if w in model.wv.vocab:\n",
      "<ipython-input-35-9fae6c049214>:50: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  if word in model.wv.vocab:\n",
      "<ipython-input-35-9fae6c049214>:52: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  if w in model.wv.vocab:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250 250\n",
      "MSE: 0.08375136343654033\n",
      "Pearson: 58.75364267134348 Spearman: 61.503226607079874\n"
     ]
    }
   ],
   "source": [
    "file=open('resultNP_VP_compare12ar.txt','w')\n",
    "\n",
    "def partitioning(sentence):\n",
    "    sen=''\n",
    "    for word in sentence:\n",
    "        for c in string.punctuation:\n",
    "            word=word.replace(c,\"\")\n",
    "        sen+=word\n",
    "    sentence=sen\n",
    "    doc=nlp(sen)\n",
    "    nounp=[]\n",
    "    verbp=[]\n",
    "    nounp=[chunk.text for chunk in doc.noun_chunks]\n",
    "    verbp=[token.lemma_ for token in doc if token.pos_ == \"VERB\"]\n",
    "    return nounp,verbp\n",
    "\n",
    "def listtostr(phrase):\n",
    "    sent=''\n",
    "    for word in phrase:\n",
    "        sent=sent+\" \"+str(word.strip())\n",
    "    return sent\n",
    "\n",
    "def result(s1,s2):\n",
    "    s1word=[]\n",
    "    s2word=[]\n",
    "    for word in s1:\n",
    "        if word not in sw:\n",
    "            s1word.append(word)\n",
    "    for word in s2:\n",
    "        if word not in sw:\n",
    "            s2word.append(word)\n",
    "            \n",
    "    #print(s1word,s2word)\n",
    "    if len(s1word)==0 or len(s2word)==0:\n",
    "        aver=0\n",
    "    else:\n",
    "        max_score=[]\n",
    "        for word in s1word:\n",
    "            score=[0]\n",
    "            if word in model.wv.vocab:\n",
    "                for w in s2word:\n",
    "                    if w in model.wv.vocab:\n",
    "                        score.append(model.similarity(word,w))\n",
    "            max_score.append(max(score))\n",
    "        #print(max_score)\n",
    "    \n",
    "        max_score2=[]\n",
    "        for word in s2word:\n",
    "            score=[0]\n",
    "            if word in model.wv.vocab:\n",
    "                for w in s1word:\n",
    "                    if w in model.wv.vocab:\n",
    "                        score.append(model.similarity(word,w))\n",
    "            max_score2.append(max(score))\n",
    "        #print(max_score2)\n",
    "    \n",
    "        average=sum(max_score)/(len(max_score))\n",
    "    \n",
    "        average1=sum(max_score2)/(len(max_score2))\n",
    "    \n",
    "        aver=(average+average1)/2\n",
    "    return aver\n",
    "\n",
    "ns1=[]\n",
    "ns2=[]\n",
    "i=0\n",
    "print('Length:',len(s1))\n",
    "while(i<len(s1)):\n",
    "    ns1=s1[i]\n",
    "    #print(ns1)\n",
    "    npp,vpp=partitioning(ns1)\n",
    "    #print(npp,vpp)\n",
    "    sen1np=listtostr(npp).strip()\n",
    "    sen1np=sen1np.split(' ')\n",
    "    #print(sen1np, len(sen1np))\n",
    "    sen1vp=listtostr(vpp).strip()\n",
    "    sen1vp=sen1vp.split(' ')\n",
    "        \n",
    "    ns2=s2[i]\n",
    "    #print(ns2)\n",
    "    npp,vpp=partitioning(ns2)\n",
    "    #print(npp,vpp)\n",
    "    sen2np=listtostr(npp).strip()\n",
    "    sen2np=sen2np.split(' ')\n",
    "    sen2vp=listtostr(vpp).strip()\n",
    "    sen2vp=sen2vp.split(' ')\n",
    "    \n",
    "    avernp=result(sen1np,sen2np)\n",
    "    #print('NP:',avernp)\n",
    "    avervp=result(sen1vp,sen2vp)\n",
    "    #print('VP:',avervp)\n",
    "    #if(avernp==0 or avervp==0):\n",
    "     #   av=(avernp+avervp)*5\n",
    "    #else:\n",
    "    av=avernp*4+avervp\n",
    "    av=str(round(av,6))+'\\n'\n",
    "    #print('No:',i+1,'similarity:',av)\n",
    "    file.writelines(av)\n",
    "    \n",
    "    i=i+1\n",
    "    \n",
    "file.close()\n",
    "\n",
    "data1=[]\n",
    "data2=[]\n",
    "f1=open('resultNP_VP_compare12ar.txt')\n",
    "f2=open('STS.gs.track2.ar-en.txt')\n",
    "def listvalue(file):\n",
    "    value=[]\n",
    "    for line in file:\n",
    "        value.append(float(line)/5)\n",
    "    return value\n",
    "data1=listvalue(f1)\n",
    "data2=listvalue(f2)\n",
    "print(len(data1),len(data2))\n",
    "mse=np.square(np.subtract(data2,data1)).mean()\n",
    "print(\"MSE:\",mse)\n",
    "corr,_=pearsonr(data1,data2)\n",
    "scor,_=spearmanr(data1,data2)\n",
    "print('Pearson:',corr*100,'Spearman:',scor*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length: 250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-36-82ad426102cd>:34: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  if k[0] in model.wv.vocab:\n",
      "<ipython-input-36-82ad426102cd>:35: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  if j[0] in model.wv.vocab:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250 250\n",
      "MSE: 0.08890595816937903\n",
      "Pearson: 52.29730283673082 Spearman: 52.8396120057849\n"
     ]
    }
   ],
   "source": [
    "file=open('resultpos1ar.txt','w')\n",
    "\n",
    "ns1=[]\n",
    "ns2=[]\n",
    "i=0\n",
    "print('Length:',len(s1))\n",
    "while(i<len(s1)):\n",
    "    ns1=s1[i].split(' ')\n",
    "    s1word=[]\n",
    "    for word in ns1:\n",
    "        for c in string.punctuation or c.isspace():\n",
    "            word=word.replace(c,'')\n",
    "        if word not in sw:\n",
    "            s1word.append(word)\n",
    "    #print(i+1,s1word)\n",
    "    \n",
    "    s2word=[]\n",
    "    ns2=s2[i].split(' ')\n",
    "    for word in ns2:\n",
    "        for c in string.punctuation:\n",
    "                word=word.replace(c,\"\")\n",
    "        if word not in sw:\n",
    "            s2word.append(word)\n",
    "    #print(i+1, s2word)\n",
    "    tag1=nltk.pos_tag(s1word,tagset='universal')\n",
    "    tag2=nltk.pos_tag(s2word,tagset='universal')\n",
    "    #print(tag1,tag2)\n",
    "    k=0\n",
    "    max_score=[]\n",
    "    for k in tag1:\n",
    "        sscore=[]\n",
    "        for j in tag2:\n",
    "            if (k[1]==j[1]):\n",
    "                if k[0] in model.wv.vocab:\n",
    "                    if j[0] in model.wv.vocab:\n",
    "                        a=model.similarity(k[0],j[0])\n",
    "                        #print(k[0],j[0],a)\n",
    "                        sscore.append(a)\n",
    "                    #print(sscore)\n",
    "        if(len(sscore)>=1):\n",
    "            max_score.append(max(sscore))\n",
    "    #print(max_score)\n",
    "    count=0\n",
    "    summ=0\n",
    "    for val in max_score:\n",
    "        summ+=val\n",
    "        count+=1\n",
    "    average=(summ*5)/count\n",
    "    #print('i:',i+1,average)\n",
    "    a=str(round(average,6))+'\\n'\n",
    "    file.writelines(a)\n",
    "    i+=1\n",
    "file.close()\n",
    "\n",
    "data1=[]\n",
    "data2=[]\n",
    "f1=open('resultpos1ar.txt')\n",
    "f2=open('STS.gs.track2.ar-en.txt')\n",
    "def listvalue(file):\n",
    "    value=[]\n",
    "    for line in file:\n",
    "        value.append(float(line)/5)\n",
    "    return value\n",
    "data1=listvalue(f1)\n",
    "data2=listvalue(f2)\n",
    "print(len(data1),len(data2))\n",
    "mse = np.square(np.subtract(data2,data1)).mean()\n",
    "print('MSE:',mse)\n",
    "corr,_=pearsonr(data1,data2)\n",
    "scor,_=spearmanr(data1,data2)\n",
    "print('Pearson:',corr*100,'Spearman:',scor*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length: 250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-37-944ed4b6d970>:23: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  if(word in model.wv.vocab):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250 250\n",
      "Pearson: 62.739439390633 Spearman: 63.92569566511496\n"
     ]
    }
   ],
   "source": [
    "file=open('newNpVpar.txt','w')\n",
    "\n",
    "def partitioning(sentence):\n",
    "    sen=''\n",
    "    for word in sentence:\n",
    "        for c in string.punctuation:\n",
    "            word=word.replace(c,\"\")\n",
    "        sen+=word\n",
    "    sentence=sen\n",
    "    doc=nlp(sen)\n",
    "    nounp=[]\n",
    "    verbp=[]\n",
    "    nounp=[chunk.text for chunk in doc.noun_chunks]\n",
    "    verbp=[token.lemma_ for token in doc if token.pos_ == \"VERB\"]\n",
    "    return nounp,verbp\n",
    "\n",
    "def vectorvalue(subsent):\n",
    "    #print(subsent)\n",
    "    subsent=subsent.split(\" \")\n",
    "    sum1=np.zeros((300,), dtype=\"float32\")\n",
    "    for word in subsent:\n",
    "        if word not in sw:\n",
    "            if(word in model.wv.vocab):\n",
    "                sum1=np.add(sum1,model[word])\n",
    "    if len(subsent)>0:\n",
    "        average1=np.divide(sum1,len(subsent),dtype=\"float32\")\n",
    "    else:\n",
    "        average1=sum1\n",
    "    return average1\n",
    "\n",
    "def listvalue(pp):\n",
    "    pplist=[]\n",
    "    if(len(pp)>0):\n",
    "        j=0\n",
    "        while(j<len(pp)):\n",
    "            pplist.append(vectorvalue(pp[j]))\n",
    "            j=j+1\n",
    "    else:\n",
    "        pplist.append(np.zeros((300,), dtype=\"float32\"))\n",
    "    return pplist\n",
    "\n",
    "def score(nplist,np2list,vplist,vp2list,a):\n",
    "    simlist=[]\n",
    "    if(np.all(nplist)==0 and np.all(np2list)==0 and np.all(vplist)==0 and np.all(vp2list)==0):\n",
    "        aa=0\n",
    "    else:\n",
    "        for value in nplist:\n",
    "            npsim=[]\n",
    "            for value2 in np2list:\n",
    "                sim=(1 - spatial.distance.cosine(value,value2))*(5/a)\n",
    "                if( np.isnan(sim)==1):\n",
    "                    sim=0\n",
    "                npsim.append(sim)\n",
    "            #print('np:',max(npsim))\n",
    "            simlist.append(max(npsim))\n",
    "        \n",
    "        for v in vplist:\n",
    "            vpsim=[]\n",
    "            for v2 in vp2list:\n",
    "                sim=(1-spatial.distance.cosine(v,v2))*(5/a)\n",
    "                if np.isnan(sim)==1:\n",
    "                    sim=0\n",
    "                vpsim.append(sim)\n",
    "            #print('vp:',max(vpsim))\n",
    "            simlist.append(max(vpsim))\n",
    "        if(len(simlist)>0):    \n",
    "            bb=max(simlist)\n",
    "            p=0\n",
    "            while(p<len(simlist)):\n",
    "                if simlist[p]<bb/2:\n",
    "                    simlist[p]=0\n",
    "                p=p+1\n",
    "        aa = sum(simlist)\n",
    "    return aa\n",
    "\n",
    "ns1=[]\n",
    "ns2=[]\n",
    "i=0\n",
    "print('Length:',len(s1))\n",
    "while(i<len(s1)):\n",
    "    ns1=s1[i]\n",
    "    #print(ns1)\n",
    "    nplist=[]\n",
    "    vplist=[]\n",
    "    np2list=[]\n",
    "    vp2list=[]\n",
    "    npp,vpp=partitioning(ns1)\n",
    "    #print(npp,vpp)\n",
    "    nplist=listvalue(npp)\n",
    "    vplist=listvalue(vpp)\n",
    "    a=len(npp)+len(vpp)\n",
    "    \n",
    "    ns2=s2[i]\n",
    "    #print(ns2)\n",
    "    npp2,vpp2=partitioning(ns2)\n",
    "    #print(npp2,vpp2)\n",
    "    np2list=listvalue(npp2)\n",
    "    vp2list=listvalue(vpp2)\n",
    "    b=len(npp2)+len(vpp2)\n",
    "    \n",
    "    if a>=b:\n",
    "        sim=score(nplist,np2list,vplist,vp2list,a)\n",
    "    else:\n",
    "        sim=score(np2list,nplist,vp2list,vplist,b)\n",
    "    #print(i+1,'similarity:',sim)\n",
    "    sim=str(round(sim,2))+'\\n'\n",
    "    file.writelines(sim)\n",
    "    i=i+1\n",
    "    \n",
    "file.close()\n",
    "\n",
    "data1=[]\n",
    "data2=[]\n",
    "f2=open('STS.gs.track2.ar-en.txt')\n",
    "f1=open('newNpVpar.txt')\n",
    "#f2=open('newNpVp.txt')\n",
    "\n",
    "def listvalue(file):\n",
    "    value=[]\n",
    "    for line in file:\n",
    "        value.append(float(line)/5)\n",
    "    return value\n",
    "data1=listvalue(f1)\n",
    "data2=listvalue(f2)\n",
    "print(len(data1),len(data2))\n",
    "mse = np.square(np.subtract(data2,data1)).mean()\n",
    "#print('MSE:',mse)\n",
    "corr,_=pearsonr(data1,data2)\n",
    "scor,_=spearmanr(data1,data2)\n",
    "print('Pearson:',corr*100,'Spearman:',scor*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length: 250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-38-df12a24ee299>:14: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  if(word in model.wv.vocab):\n",
      "<ipython-input-38-df12a24ee299>:41: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  if(word in model.wv.vocab):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson: 67.82589916042335 Spearman: 67.96456310723458\n"
     ]
    }
   ],
   "source": [
    "file=open('new_vector_formar.txt','w')\n",
    "ns1=[]\n",
    "ns2=[]\n",
    "i=0\n",
    "print('Length:',len(s1))\n",
    "while(i<250):\n",
    "    ns1=s1[i].split(' ')\n",
    "    sum1=np.zeros((300,), dtype=\"float32\")\n",
    "    w=0\n",
    "    for word in ns1:\n",
    "        for c in string.punctuation:\n",
    "                word=word.replace(c,\"\")\n",
    "        if word not in sw:\n",
    "            if(word in model.wv.vocab):\n",
    "                if w==0:\n",
    "                    sum1=np.add(sum1,model[word])\n",
    "                    w=w+1\n",
    "                else:\n",
    "                    av=sum(sum1)/300\n",
    "                    a=list(model[word])\n",
    "                    sum1=list(sum1)\n",
    "                    k=0\n",
    "                    while(k<300):\n",
    "                        if abs(a[k]-sum1[k])>=0.16:   #works better at 0.135\n",
    "                            sum1[k]=a[k]+sum1[k]\n",
    "                            k=k+1\n",
    "                        else:\n",
    "                            sum1[k]=sum1[k]\n",
    "                            k=k+1\n",
    "                    sum1=np.asarray(sum1)\n",
    "    average1=np.divide(sum1,1,dtype=\"float32\")\n",
    "            \n",
    "    ns2=s2[i].split(' ')\n",
    "    sum2=np.zeros((300,),dtype=\"float32\")\n",
    "    \n",
    "    w=0\n",
    "    for word in ns2:\n",
    "        for c in string.punctuation:\n",
    "                word=word.replace(c,\"\")\n",
    "        if word not in sw:\n",
    "            if(word in model.wv.vocab):\n",
    "                if w==0:\n",
    "                    sum2=np.add(sum2,model[word])\n",
    "                    w=w+1\n",
    "                else:\n",
    "                    av=sum(sum2)/300\n",
    "                    a=list(model[word])\n",
    "                    sum2=list(sum2)\n",
    "                    k=0\n",
    "                    while(k<300):\n",
    "                        if abs(a[k]-sum2[k])>=0.16:    # better at 0.16--- 73.17\n",
    "                            sum2[k]=a[k]+sum2[k]\n",
    "                            k=k+1\n",
    "                        else:\n",
    "                            sum2[k]=sum2[k]\n",
    "                            k=k+1\n",
    "                    sum2=np.asarray(sum2)        \n",
    "    average2=np.divide(sum2,1,dtype=\"float32\")\n",
    "\n",
    "    sim = 1 - spatial.distance.cosine(average1,average2)\n",
    "    sim=sim*5\n",
    "    a=str(round(sim,6))+'\\n'\n",
    "    file.writelines(a)\n",
    "    #print(i,'similarity',sim)\n",
    "    i+=1\n",
    "file.close()\n",
    "\n",
    "\n",
    "data1=[]\n",
    "data2=[]\n",
    "f1=open('new_vector_formar.txt')\n",
    "f2=open('STS.gs.track2.ar-en.txt')\n",
    "def listvalue(file):\n",
    "    value=[]\n",
    "    for line in file:\n",
    "        value.append(float(line)/5)\n",
    "    return value\n",
    "data1=listvalue(f1)\n",
    "data2=listvalue(f2)\n",
    "corr,_=pearsonr(data1,data2)\n",
    "scor,_=spearmanr(data1,data2)\n",
    "print('Pearson:',corr*100,'Spearman:',scor*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length: 250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-39-8f2113a9003e>:23: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  if(word in model.wv.vocab):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250 250\n",
      "MSE: 0.10011280000000002\n",
      "Pearson: 57.036624541751465 Spearman: 61.221899624587316\n"
     ]
    }
   ],
   "source": [
    "file=open('resultNP_VP12ar.txt','w')\n",
    "\n",
    "def partitioning(sentence):\n",
    "    sen=''\n",
    "    for word in sentence:\n",
    "        for c in string.punctuation:\n",
    "            word=word.replace(c,\"\")\n",
    "        sen+=word\n",
    "    sentence=sen\n",
    "    doc=nlp(sen)\n",
    "    nounp=[]\n",
    "    verbp=[]\n",
    "    nounp=[chunk.text for chunk in doc.noun_chunks]\n",
    "    verbp=[token.lemma_ for token in doc if token.pos_ == \"VERB\"]\n",
    "    return nounp,verbp\n",
    "\n",
    "def vectorvalue(subsent):\n",
    "    #print(subsent)\n",
    "    subsent=subsent.split(\" \")\n",
    "    sum1=np.zeros((300,), dtype=\"float32\")\n",
    "    for word in subsent:\n",
    "        if word not in sw:\n",
    "            if(word in model.wv.vocab):\n",
    "                sum1=np.add(sum1,model[word])\n",
    "    if len(subsent)>0:\n",
    "        average1=np.divide(sum1,len(subsent),dtype=\"float32\")\n",
    "    else:\n",
    "        average1=sum1\n",
    "    return average1\n",
    "\n",
    "def listtostr(phrase):\n",
    "    sent=''\n",
    "    for word in phrase:\n",
    "        sent=sent+\" \"+str(word)\n",
    "    return vectorvalue(sent)\n",
    "\n",
    "ns1=[]\n",
    "ns2=[]\n",
    "i=0\n",
    "print('Length:',len(s1))\n",
    "while(i<len(s1)):\n",
    "    ns1=s1[i]\n",
    "    #print(ns1)\n",
    "    npp,vpp=partitioning(ns1)\n",
    "    #print(npp,vpp)\n",
    "    sen1np=listtostr(npp)\n",
    "    sen1vp=listtostr(vpp)\n",
    "        \n",
    "    ns2=s2[i]\n",
    "    #print(ns2)\n",
    "    npp,vpp=partitioning(ns2)\n",
    "    #print(npp,vpp)\n",
    "    sen2np=listtostr(npp)\n",
    "    sen2vp=listtostr(vpp)\n",
    "        \n",
    "    if ((np.all(sen1np==0)) or (np.all(sen2np==0))):\n",
    "            nppsim=0\n",
    "    else:\n",
    "        nppsim= 1 - spatial.distance.cosine(sen1np,sen2np)\n",
    "    #print('nounphase:',nppsim)\n",
    "    if ((np.all(sen1vp==0)) or (np.all(sen2vp==0))):\n",
    "            vppsim=0\n",
    "    else:\n",
    "        vppsim=1 - spatial.distance.cosine(sen1vp,sen2vp)\n",
    "    similarity=nppsim*4+vppsim\n",
    "    sim=str(round(similarity,2))+'\\n'\n",
    "    file.writelines(sim)\n",
    "    i+=1\n",
    "    \n",
    "file.close()\n",
    "\n",
    "data1=[]\n",
    "data2=[]\n",
    "f1=open('resultNP_VP12ar.txt')\n",
    "f2=open('STS.gs.track2.ar-en.txt')\n",
    "def listvalue(file):\n",
    "    value=[]\n",
    "    for line in file:\n",
    "        value.append(float(line)/5)\n",
    "    return value\n",
    "data1=listvalue(f1)\n",
    "data2=listvalue(f2)\n",
    "print(len(data1),len(data2))\n",
    "mse = np.square(np.subtract(data2,data1)).mean()\n",
    "print('MSE:',mse)\n",
    "corr,_=pearsonr(data1,data2)\n",
    "scor,_=spearmanr(data1,data2)\n",
    "print('Pearson:',corr*100,'Spearman:',scor*100)\n",
    "#P=67.51, S=70.18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
