{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "import numpy as np\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.stats import spearmanr\n",
    "from scipy import spatial\n",
    "import spacy\n",
    "nlp=spacy.load('en_core_web_sm')\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pretrained Word2Vec model. It was pretrained on Google News Corpus\n",
    "filename = 'GoogleNews-vectors-negative300.bin'\n",
    "model = KeyedVectors.load_word2vec_format(filename, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "sw=stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STS-2017 Dataset is used here.Here, we are taking the first sentence and second sentence into two list. \n",
    "fh=open('STS.input.track51.en-en.txt')\n",
    "c=0\n",
    "s1=[]\n",
    "s2=[]\n",
    "for line in fh:\n",
    "    line=line.lower()\n",
    "    c+=1\n",
    "    line=line.rstrip()\n",
    "    sentences=line.split('\\t')\n",
    "    s1.append(sentences[0])\n",
    "    s2.append(sentences[1]) \n",
    "print(\"No of Sentence pair:\", c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file=open('new_vector_form_auto.txt','w')\n",
    "ns1=[]\n",
    "ns2=[]\n",
    "i=0\n",
    "print('Length:',len(s1))\n",
    "while(i<250):\n",
    "    ns1=s1[i].split(' ')\n",
    "    sum1=np.zeros((300,), dtype=\"float32\")\n",
    "    w=0\n",
    "    for word in ns1:\n",
    "        for c in string.punctuation:\n",
    "                word=word.replace(c,\"\")\n",
    "        if word not in sw:\n",
    "            if(word in model.wv.vocab):\n",
    "                if w==0:\n",
    "                    sum1=np.add(sum1,model[word])\n",
    "                    w=w+1\n",
    "                else:\n",
    "                    av=sum(sum1)/300\n",
    "                    a=list(model[word])\n",
    "                    sum1=list(sum1)\n",
    "                    m=max(sum1) #extra automation\n",
    "                    k=0\n",
    "                    while(k<300):\n",
    "                        if abs(a[k]-sum1[k])>= (m/8): # works better at 0.135 --> 79.32\n",
    "                            sum1[k]=a[k]+sum1[k]\n",
    "                            k=k+1\n",
    "                        else:\n",
    "                            sum1[k]=sum1[k]\n",
    "                            k=k+1\n",
    "                    sum1=np.asarray(sum1)\n",
    "    average1=np.divide(sum1,1,dtype=\"float32\")\n",
    "            \n",
    "    ns2=s2[i].split(' ')\n",
    "    sum2=np.zeros((300,),dtype=\"float32\")\n",
    "    \n",
    "    w=0\n",
    "    for word in ns2:\n",
    "        for c in string.punctuation:\n",
    "                word=word.replace(c,\"\")\n",
    "        if word not in sw:\n",
    "            if(word in model.wv.vocab):\n",
    "                if w==0:\n",
    "                    sum2=np.add(sum2,model[word])\n",
    "                    w=w+1\n",
    "                else:\n",
    "                    av=sum(sum2)/300\n",
    "                    a=list(model[word])\n",
    "                    sum2=list(sum2)\n",
    "                    m2=max(sum2)\n",
    "                    k=0\n",
    "                    while(k<300):\n",
    "                        if abs(a[k]-sum2[k])>= (m2/8): # works better at 0.135 --> 79.32\n",
    "                            sum2[k]=a[k]+sum2[k]\n",
    "                            k=k+1\n",
    "                        else:\n",
    "                            sum2[k]=sum2[k]\n",
    "                            k=k+1\n",
    "                    sum2=np.asarray(sum2)        \n",
    "    average2=np.divide(sum2,1,dtype=\"float32\")\n",
    "\n",
    "    sim = 1 - spatial.distance.cosine(average1,average2)\n",
    "    sim=sim*5\n",
    "    a=str(round(sim,6))+'\\n'\n",
    "    file.writelines(a)\n",
    "    #print(i,'similarity',sim)\n",
    "    i+=1\n",
    "file.close()\n",
    "\n",
    "data1=[]\n",
    "data2=[]\n",
    "f1=open('new_vector_form_auto.txt')\n",
    "f2=open('STS.gs.track5.en-en.txt')\n",
    "def listvalue(file):\n",
    "    value=[]\n",
    "    for line in file:\n",
    "        value.append(float(line)/5)\n",
    "    return value\n",
    "data1=listvalue(f1)\n",
    "data2=listvalue(f2)\n",
    "corr,_=pearsonr(data1,data2)\n",
    "scor,_=spearmanr(data1,data2)\n",
    "print('FThres_sim:')\n",
    "print('Pearson:',corr*100,'Spearman:',scor*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STS-2017 Dataset is used here.Here, we are taking the first sentence and second sentence into two list. \n",
    "fh=open('spanish_english_data.txt')\n",
    "c=0\n",
    "s1=[]\n",
    "s2=[]\n",
    "for line in fh:\n",
    "    line=line.lower()\n",
    "    c+=1\n",
    "    line=line.rstrip()\n",
    "    sentences=sent_text = sent_tokenize(line)\n",
    "    #print(c, sentences)\n",
    "    s1.append(sentences[0])\n",
    "    s2.append(sentences[1]) \n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file=open('new_vector_formsp_auto.txt','w')\n",
    "ns1=[]\n",
    "ns2=[]\n",
    "i=0\n",
    "print('Length:',len(s1))\n",
    "while(i<250):\n",
    "    ns1=s1[i].split(' ')\n",
    "    sum1=np.zeros((300,), dtype=\"float32\")\n",
    "    w=0\n",
    "    for word in ns1:\n",
    "        for c in string.punctuation:\n",
    "                word=word.replace(c,\"\")\n",
    "        if word not in sw:\n",
    "            if(word in model.wv.vocab):\n",
    "                if w==0:\n",
    "                    sum1=np.add(sum1,model[word])\n",
    "                    w=w+1\n",
    "                else:\n",
    "                    av=sum(sum1)/300\n",
    "                    a=list(model[word])\n",
    "                    sum1=list(sum1)\n",
    "                    m=max(sum1)\n",
    "                    k=0\n",
    "                    while(k<300):\n",
    "                        if abs(a[k]-sum1[k])>=(m/6): #works better at 0.16 --> Pearson: 73.65 Spearman: 72.76\n",
    "                            sum1[k]=a[k]+sum1[k]\n",
    "                            k=k+1\n",
    "                        else:\n",
    "                            sum1[k]=sum1[k]\n",
    "                            k=k+1\n",
    "                    sum1=np.asarray(sum1)\n",
    "    average1=np.divide(sum1,1,dtype=\"float32\")\n",
    "            \n",
    "    ns2=s2[i].split(' ')\n",
    "    sum2=np.zeros((300,),dtype=\"float32\")\n",
    "    \n",
    "    w=0\n",
    "    for word in ns2:\n",
    "        for c in string.punctuation:\n",
    "                word=word.replace(c,\"\")\n",
    "        if word not in sw:\n",
    "            if(word in model.wv.vocab):\n",
    "                if w==0:\n",
    "                    sum2=np.add(sum2,model[word])\n",
    "                    w=w+1\n",
    "                else:\n",
    "                    av=sum(sum2)/300\n",
    "                    a=list(model[word])\n",
    "                    sum2=list(sum2)\n",
    "                    m2=max(sum2)\n",
    "                    k=0\n",
    "                    while(k<300):\n",
    "                        if abs(a[k]-sum2[k])>=(m2/6):\n",
    "                            sum2[k]=a[k]+sum2[k]\n",
    "                            k=k+1\n",
    "                        else:\n",
    "                            sum2[k]=sum2[k]\n",
    "                            k=k+1\n",
    "                    sum2=np.asarray(sum2)        \n",
    "    average2=np.divide(sum2,1,dtype=\"float32\")\n",
    "\n",
    "    sim = 1 - spatial.distance.cosine(average1,average2)\n",
    "    sim=sim*5\n",
    "    a=str(round(sim,6))+'\\n'\n",
    "    file.writelines(a)\n",
    "    #print(i,'similarity',sim)\n",
    "    i+=1\n",
    "file.close()\n",
    "\n",
    "\n",
    "data1=[]\n",
    "data2=[]\n",
    "f1=open('new_vector_formsp_auto.txt')\n",
    "f2=open('STS.gs.track4a.es-en.txt')\n",
    "def listvalue(file):\n",
    "    value=[]\n",
    "    for line in file:\n",
    "        value.append(float(line)/5)\n",
    "    return value\n",
    "data1=listvalue(f1)\n",
    "data2=listvalue(f2)\n",
    "corr,_=pearsonr(data1,data2)\n",
    "scor,_=spearmanr(data1,data2)\n",
    "print('Pearson:',corr*100,'Spearman:',scor*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STS-2017 Dataset is used here.Here, we are taking the first sentence and second sentence into two list. \n",
    "fh=open('turkish_english_data.txt')\n",
    "c=0\n",
    "s1=[]\n",
    "s2=[]\n",
    "for line in fh:\n",
    "    line=line.lower()\n",
    "    c+=1\n",
    "    line=line.rstrip()\n",
    "    #print(c)\n",
    "    sentences=sent_text = sent_tokenize(line)\n",
    "    #print(c, sentences)\n",
    "    s1.append(sentences[0])\n",
    "    s2.append(sentences[1]) \n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file=open('new_vector_formtr_auto.txt','w')\n",
    "ns1=[]\n",
    "ns2=[]\n",
    "i=0\n",
    "print('Length:',len(s1))\n",
    "while(i<250):\n",
    "    ns1=s1[i].split(' ')\n",
    "    sum1=np.zeros((300,), dtype=\"float32\")\n",
    "    w=0\n",
    "    for word in ns1:\n",
    "        for c in string.punctuation:\n",
    "                word=word.replace(c,\"\")\n",
    "        if word not in sw:\n",
    "            if(word in model.wv.vocab):\n",
    "                if w==0:\n",
    "                    sum1=np.add(sum1,model[word])\n",
    "                    w=w+1\n",
    "                else:\n",
    "                    av=sum(sum1)/300\n",
    "                    a=list(model[word])\n",
    "                    sum1=list(sum1)\n",
    "                    m=max(sum1)\n",
    "                    k=0\n",
    "                    while(k<300):\n",
    "                        if abs(a[k]-sum1[k])>=(m/4.5): #works better at 0.18 --> pearson:68.41 Spearman: 68.15   \n",
    "                            sum1[k]=a[k]+sum1[k]\n",
    "                            k=k+1\n",
    "                        else:\n",
    "                            sum1[k]=sum1[k]\n",
    "                            k=k+1\n",
    "                    sum1=np.asarray(sum1)\n",
    "    average1=np.divide(sum1,1,dtype=\"float32\")\n",
    "            \n",
    "    ns2=s2[i].split(' ')\n",
    "    sum2=np.zeros((300,),dtype=\"float32\")\n",
    "    \n",
    "    w=0\n",
    "    for word in ns2:\n",
    "        for c in string.punctuation:\n",
    "                word=word.replace(c,\"\")\n",
    "        if word not in sw:\n",
    "            if(word in model.wv.vocab):\n",
    "                if w==0:\n",
    "                    sum2=np.add(sum2,model[word])\n",
    "                    w=w+1\n",
    "                else:\n",
    "                    av=sum(sum2)/300\n",
    "                    a=list(model[word])\n",
    "                    sum2=list(sum2)\n",
    "                    m2=max(sum2)\n",
    "                    k=0\n",
    "                    while(k<300):\n",
    "                        if abs(a[k]-sum2[k])>=(m2/4.5): \n",
    "                            sum2[k]=a[k]+sum2[k]\n",
    "                            k=k+1\n",
    "                        else:\n",
    "                            sum2[k]=sum2[k]\n",
    "                            k=k+1\n",
    "                    sum2=np.asarray(sum2)        \n",
    "    average2=np.divide(sum2,1,dtype=\"float32\")\n",
    "\n",
    "    sim = 1 - spatial.distance.cosine(average1,average2)\n",
    "    sim=sim*5\n",
    "    a=str(round(sim,6))+'\\n'\n",
    "    file.writelines(a)\n",
    "    #print(i,'similarity',sim)\n",
    "    i+=1\n",
    "file.close()\n",
    "\n",
    "\n",
    "data1=[]\n",
    "data2=[]\n",
    "f1=open('new_vector_formtr_auto.txt')\n",
    "f2=open('STS.gs.track6.tr-en.txt')\n",
    "def listvalue(file):\n",
    "    value=[]\n",
    "    for line in file:\n",
    "        value.append(float(line)/5)\n",
    "    return value\n",
    "data1=listvalue(f1)\n",
    "data2=listvalue(f2)\n",
    "corr,_=pearsonr(data1,data2)\n",
    "scor,_=spearmanr(data1,data2)\n",
    "print(\"FThres_sim:\")\n",
    "print('Pearson:',corr*100,'Spearman:',scor*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STS-2017 Dataset is used here.Here, we are taking the first sentence and second sentence into two list. \n",
    "fh=open('arabic_english_data.txt')\n",
    "c=0\n",
    "s1=[]\n",
    "s2=[]\n",
    "for line in fh:\n",
    "    line=line.lower()\n",
    "    c+=1\n",
    "    line=line.rstrip()\n",
    "    #print(c)\n",
    "    sentences=sent_text = sent_tokenize(line)\n",
    "    #print(c, sentences)\n",
    "    s1.append(sentences[0])\n",
    "    s2.append(sentences[1]) \n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file=open('new_vector_formar_auto.txt','w')\n",
    "ns1=[]\n",
    "ns2=[]\n",
    "i=0\n",
    "print('Length:',len(s1))\n",
    "while(i<250):\n",
    "    ns1=s1[i].split(' ')\n",
    "    sum1=np.zeros((300,), dtype=\"float32\")\n",
    "    w=0\n",
    "    for word in ns1:\n",
    "        for c in string.punctuation:\n",
    "                word=word.replace(c,\"\")\n",
    "        if word not in sw:\n",
    "            if(word in model.wv.vocab):\n",
    "                if w==0:\n",
    "                    sum1=np.add(sum1,model[word])\n",
    "                    w=w+1\n",
    "                else:\n",
    "                    av=sum(sum1)/300\n",
    "                    a=list(model[word])\n",
    "                    sum1=list(sum1)\n",
    "                    m=max(sum1)\n",
    "                    k=0\n",
    "                    while(k<300):\n",
    "                        if abs(a[k]-sum1[k])>=(m/6): # works better at 0.16 p=67.83 and Spearman=67.94\n",
    "                            sum1[k]=a[k]+sum1[k]\n",
    "                            k=k+1\n",
    "                        else:\n",
    "                            sum1[k]=sum1[k]\n",
    "                            k=k+1\n",
    "                    sum1=np.asarray(sum1)\n",
    "    average1=np.divide(sum1,1,dtype=\"float32\")\n",
    "            \n",
    "    ns2=s2[i].split(' ')\n",
    "    sum2=np.zeros((300,),dtype=\"float32\")\n",
    "    \n",
    "    w=0\n",
    "    for word in ns2:\n",
    "        for c in string.punctuation:\n",
    "                word=word.replace(c,\"\")\n",
    "        if word not in sw:\n",
    "            if(word in model.wv.vocab):\n",
    "                if w==0:\n",
    "                    sum2=np.add(sum2,model[word])\n",
    "                    w=w+1\n",
    "                else:\n",
    "                    av=sum(sum2)/300\n",
    "                    a=list(model[word])\n",
    "                    sum2=list(sum2)\n",
    "                    m2=max(sum2)\n",
    "                    k=0\n",
    "                    while(k<300):\n",
    "                        if abs(a[k]-sum2[k])>=(m2/6):    \n",
    "                            sum2[k]=a[k]+sum2[k]\n",
    "                            k=k+1\n",
    "                        else:\n",
    "                            sum2[k]=sum2[k]\n",
    "                            k=k+1\n",
    "                    sum2=np.asarray(sum2)        \n",
    "    average2=np.divide(sum2,1,dtype=\"float32\")\n",
    "\n",
    "    sim = 1 - spatial.distance.cosine(average1,average2)\n",
    "    sim=sim*5\n",
    "    a=str(round(sim,6))+'\\n'\n",
    "    file.writelines(a)\n",
    "    #print(i,'similarity',sim)\n",
    "    i+=1\n",
    "file.close()\n",
    "\n",
    "\n",
    "data1=[]\n",
    "data2=[]\n",
    "f1=open('new_vector_formar_auto.txt')\n",
    "f2=open('STS.gs.track2.ar-en.txt')\n",
    "def listvalue(file):\n",
    "    value=[]\n",
    "    for line in file:\n",
    "        value.append(float(line)/5)\n",
    "    return value\n",
    "data1=listvalue(f1)\n",
    "data2=listvalue(f2)\n",
    "corr,_=pearsonr(data1,data2)\n",
    "scor,_=spearmanr(data1,data2)\n",
    "print('FThres_sim:')\n",
    "print('Pearson:',corr*100,'Spearman:',scor*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "model = SentenceTransformer('distilbert-base-nli-stsb-mean-tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a='baby'\n",
    "#vec= model.encode(a,convert_to_tensor=True)\n",
    "vec2=model.encode(a)\n",
    "print(len(vec2),type(vec2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vec2[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file=open('new_vector_form_auto_bert.txt','w')\n",
    "ns1=[]\n",
    "ns2=[]\n",
    "i=0\n",
    "print('Length:',len(s1))\n",
    "while(i<250):\n",
    "    ns1=s1[i].split(' ')\n",
    "    sum1=np.zeros((768,), dtype=\"float32\")\n",
    "    w=0\n",
    "    for word in ns1:\n",
    "        for c in string.punctuation:\n",
    "                word=word.replace(c,\"\")\n",
    "        if word not in sw:\n",
    "            if w==0:\n",
    "                sum1=np.add(sum1,model.encode(word))\n",
    "                w=w+1\n",
    "            else:\n",
    "                a=list(model.encode(word))\n",
    "                sum1=list(sum1)\n",
    "                m=max(sum1) #extra automation\n",
    "                k=0\n",
    "                while(k<768):\n",
    "                    if abs(a[k]-sum1[k])>= (m/12): # works better at \n",
    "                        sum1[k]=a[k]+sum1[k]\n",
    "                        k=k+1\n",
    "                    else:\n",
    "                        sum1[k]=sum1[k]\n",
    "                        k=k+1\n",
    "                sum1=np.asarray(sum1)\n",
    "    average1=np.divide(sum1,1,dtype=\"float32\")\n",
    "            \n",
    "    ns2=s2[i].split(' ')\n",
    "    sum2=np.zeros((768,),dtype=\"float32\")\n",
    "    \n",
    "    w=0\n",
    "    for word in ns2:\n",
    "        for c in string.punctuation:\n",
    "                word=word.replace(c,\"\")\n",
    "        if word not in sw:\n",
    "            if w==0:\n",
    "                sum2=np.add(sum2,model.encode(word))\n",
    "                w=w+1\n",
    "            else:\n",
    "                a=list(model.encode(word))\n",
    "                sum2=list(sum2)\n",
    "                m2=max(sum2)\n",
    "                k=0\n",
    "                while(k<768):\n",
    "                    if abs(a[k]-sum2[k])>= (m2/12): # works better at 0.135 --> 79.32\n",
    "                        sum2[k]=a[k]+sum2[k]\n",
    "                        k=k+1\n",
    "                    else:\n",
    "                        sum2[k]=sum2[k]\n",
    "                        k=k+1\n",
    "                sum2=np.asarray(sum2)        \n",
    "    average2=np.divide(sum2,1,dtype=\"float32\")\n",
    "\n",
    "    sim = 1 - spatial.distance.cosine(average1,average2)\n",
    "    sim=sim*5\n",
    "    a=str(round(sim,6))+'\\n'\n",
    "    file.writelines(a)\n",
    "    #print(i,'similarity',sim)\n",
    "    i+=1\n",
    "file.close()\n",
    "\n",
    "data1=[]\n",
    "data2=[]\n",
    "f1=open('new_vector_form_auto_bert.txt')\n",
    "f2=open('STS.gs.track5.en-en.txt')\n",
    "def listvalue(file):\n",
    "    value=[]\n",
    "    for line in file:\n",
    "        value.append(float(line)/5)\n",
    "    return value\n",
    "data1=listvalue(f1)\n",
    "data2=listvalue(f2)\n",
    "corr,_=pearsonr(data1,data2)\n",
    "scor,_=spearmanr(data1,data2)\n",
    "print('FThres_sim:')\n",
    "print('Pearson:',corr*100,'Spearman:',scor*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file=open('new_vector_form_auto_bert2.txt','w')\n",
    "ns1=[]\n",
    "ns2=[]\n",
    "i=0\n",
    "print('Length:',len(s1))\n",
    "while(i<250):\n",
    "    ns1=s1[i].split(' ')\n",
    "    sum1=np.zeros((768,), dtype=\"float32\")\n",
    "    w=0\n",
    "    for word in ns1:\n",
    "        for c in string.punctuation:\n",
    "                word=word.replace(c,\"\")\n",
    "        if word not in sw:\n",
    "            if w==0:\n",
    "                sum1=np.add(sum1,model.encode(word))\n",
    "                w=w+1\n",
    "            else:\n",
    "                a=list(model.encode(word))\n",
    "                sum1=list(sum1)\n",
    "                di=np.std(a)\n",
    "                av=np.mean(a)\n",
    "                k=0\n",
    "                while(k<768):\n",
    "                    if abs(a[k]-sum1[k])>= (av+di): # works better at \n",
    "                        sum1[k]=a[k]+sum1[k]\n",
    "                        k=k+1\n",
    "                    else:\n",
    "                        sum1[k]=sum1[k]\n",
    "                        k=k+1\n",
    "                sum1=np.asarray(sum1)\n",
    "    average1=np.divide(sum1,1,dtype=\"float32\")\n",
    "            \n",
    "    ns2=s2[i].split(' ')\n",
    "    sum2=np.zeros((768,),dtype=\"float32\")\n",
    "    \n",
    "    w=0\n",
    "    for word in ns2:\n",
    "        for c in string.punctuation:\n",
    "                word=word.replace(c,\"\")\n",
    "        if word not in sw:\n",
    "            if w==0:\n",
    "                sum2=np.add(sum2,model.encode(word))\n",
    "                w=w+1\n",
    "            else:\n",
    "                a=list(model.encode(word))\n",
    "                sum2=list(sum2)\n",
    "                m2=max(sum2)\n",
    "                av2=np.mean(a)\n",
    "                di2=np.std(a)\n",
    "                k=0\n",
    "                while(k<768):\n",
    "                    if abs(a[k]-sum2[k])>= (av2+di2): \n",
    "                        sum2[k]=a[k]+sum2[k]\n",
    "                        k=k+1\n",
    "                    else:\n",
    "                        sum2[k]=sum2[k]\n",
    "                        k=k+1\n",
    "                sum2=np.asarray(sum2)        \n",
    "    average2=np.divide(sum2,1,dtype=\"float32\")\n",
    "\n",
    "    sim = 1 - spatial.distance.cosine(average1,average2)\n",
    "    sim=sim*5\n",
    "    a=str(round(sim,6))+'\\n'\n",
    "    file.writelines(a)\n",
    "    #print(i,'similarity',sim)\n",
    "    i+=1\n",
    "file.close()\n",
    "\n",
    "data1=[]\n",
    "data2=[]\n",
    "f1=open('new_vector_form_auto_bert2.txt')\n",
    "f2=open('STS.gs.track5.en-en.txt')\n",
    "def listvalue(file):\n",
    "    value=[]\n",
    "    for line in file:\n",
    "        value.append(float(line)/5)\n",
    "    return value\n",
    "data1=listvalue(f1)\n",
    "data2=listvalue(f2)\n",
    "corr,_=pearsonr(data1,data2)\n",
    "scor,_=spearmanr(data1,data2)\n",
    "print('FThres_sim:')\n",
    "print('Pearson:',corr*100,'Spearman:',scor*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file=open('new_vector_form_auto_bert3.txt','w')\n",
    "ns1=[]\n",
    "ns2=[]\n",
    "i=0\n",
    "print('Length:',len(s1))\n",
    "while(i<250):\n",
    "    ns1=s1[i].split(' ')\n",
    "    sum1=np.zeros((768,), dtype=\"float32\")\n",
    "    for word in ns1:\n",
    "        for c in string.punctuation:\n",
    "                word=word.replace(c,\"\")\n",
    "        if word not in sw:\n",
    "            sum1=np.add(sum1,model.encode(word))\n",
    "    average1=np.divide(sum1,1,dtype=\"float32\")\n",
    "            \n",
    "    ns2=s2[i].split(' ')\n",
    "    sum2=np.zeros((768,),dtype=\"float32\")\n",
    "    \n",
    "    w=0\n",
    "    for word in ns2:\n",
    "        for c in string.punctuation:\n",
    "                word=word.replace(c,\"\")\n",
    "        if word not in sw:\n",
    "            sum2=np.add(sum2,model.encode(word))\n",
    "    average2=np.divide(sum2,1,dtype=\"float32\")\n",
    "\n",
    "    sim = 1 - spatial.distance.cosine(average1,average2)\n",
    "    sim=sim*5\n",
    "    a=str(round(sim,6))+'\\n'\n",
    "    file.writelines(a)\n",
    "    #print(i,'similarity',sim)\n",
    "    i+=1\n",
    "file.close()\n",
    "\n",
    "data1=[]\n",
    "data2=[]\n",
    "f1=open('new_vector_form_auto_bert3.txt')\n",
    "f2=open('STS.gs.track5.en-en.txt')\n",
    "def listvalue(file):\n",
    "    value=[]\n",
    "    for line in file:\n",
    "        value.append(float(line)/5)\n",
    "    return value\n",
    "data1=listvalue(f1)\n",
    "data2=listvalue(f2)\n",
    "corr,_=pearsonr(data1,data2)\n",
    "scor,_=spearmanr(data1,data2)\n",
    "print('FThres_sim:')\n",
    "print('Pearson:',corr*100,'Spearman:',scor*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert(s1, s2):\n",
    "    sen= str(s1)\n",
    "    sen2=str(s2)\n",
    "    for word in s1:\n",
    "        sw1=[]\n",
    "        sw1.append(word)\n",
    "        \n",
    "sen=open(\"STS.input.track51.en-en.txt\", 'r')\n",
    "\n",
    "ss1=[]\n",
    "ss2=[]\n",
    "for line in sen:\n",
    "    line=line.rstrip()\n",
    "    sentences=line.split('\\t')\n",
    "    ss1.append(sentences[0])\n",
    "    ss2.append(sentences[1])  \n",
    "\n",
    "\n",
    "#Compute embedding for both lists\n",
    "embeddings1 = model.encode(ss1, convert_to_tensor=True)\n",
    "embeddings2 = model.encode(ss2, convert_to_tensor=True)\n",
    "\n",
    "#Compute cosine-similarits\n",
    "cosine_scores = util.pytorch_cos_sim(embeddings1, embeddings2)\n",
    "#print(cosine_scores)\n",
    "file=open(\"bert42.txt\", \"w\")\n",
    "\n",
    "#Output the pairs with their score\n",
    "for i in range(len(ss1)):\n",
    "    p=\"{}\".format(cosine_scores[i][i])\n",
    "    #sim=round(p,2)+'\\n'\n",
    "    #q=float(cosine_scores[i][i])\n",
    "   # print((p,  q))\n",
    "    file.write(p+'\\n')\n",
    "file.close()\n",
    "\n",
    "\n",
    "#measure mse, pearson, spearman\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.stats import spearmanr\n",
    "import numpy as np\n",
    "\n",
    "data1=[]\n",
    "data2=[]\n",
    "f1=open('bert42.txt')\n",
    "f2=open('STS.gs.track5.en-en.txt')\n",
    "#f2=open('gs.txt')\n",
    "def listvalue(file):\n",
    "    value=[]\n",
    "    for line in file:\n",
    "        value.append(float(line)/5)\n",
    "    return value\n",
    "data1=listvalue(f1)\n",
    "data2=listvalue(f2)\n",
    "\n",
    "def score(file1, file2):\n",
    "        corr,_=pearsonr(file1,file2)\n",
    "        scor,_=spearmanr(file1,file2)\n",
    "        print('Pearson:',round((corr*100),2),'Spearman:',round((scor*100),2))\n",
    "\n",
    "print(\"bert_sim\")\n",
    "score(data1, data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sen=open(\"STS.input.track51.en-en.txt\", 'r')\n",
    "\n",
    "s1=[]\n",
    "s2=[]\n",
    "for line in sen:\n",
    "    line=line.rstrip()\n",
    "    sentences=line.split('\\t')\n",
    "    s1.append(sentences[0])\n",
    "    s2.append(sentences[1])  \n",
    "#print(s2)\n",
    "\n",
    "def bert(s1,s2):\n",
    "    if(len(s1)==0 or len(s2)==0):\n",
    "        aver=0\n",
    "    else:\n",
    "        max_score=[]\n",
    "        for word in s1:\n",
    "            sen1=[]\n",
    "            sen1.append(word)\n",
    "            score=[0]\n",
    "            embeddings1 = model.encode(sen1, convert_to_tensor=True)\n",
    "            for word in s2:\n",
    "                sen2=[]\n",
    "                sen2.append(word)\n",
    "                embeddings2 = model.encode(sen2, convert_to_tensor=True)\n",
    "                cosine = util.pytorch_cos_sim(embeddings1, embeddings2)\n",
    "                score.append(float(cosine))\n",
    "            max_score.append(max(score))\n",
    "            \n",
    "        max_score2=[]\n",
    "        for word in s2:\n",
    "            sen2=[]\n",
    "            sen2.append(word)\n",
    "            score=[0]\n",
    "            embeddings1 = model.encode(sen2, convert_to_tensor=True)\n",
    "            for word in s1: \n",
    "                sen1=[]\n",
    "                sen1.append(word)\n",
    "                embeddings2 = model.encode(sen1, convert_to_tensor=True)\n",
    "                cosine = util.pytorch_cos_sim(embeddings1, embeddings2)\n",
    "                score.append(float(cosine))\n",
    "            max_score2.append(max(score))\n",
    "            \n",
    "            average=sum(max_score)/(len(max_score))\n",
    "    \n",
    "            average1=sum(max_score2)/(len(max_score2))\n",
    "    \n",
    "            aver=(average+average1)/2\n",
    "    return aver\n",
    "            \n",
    "    \n",
    "    \n",
    "\n",
    "fh=open('STS.input.track51.en-en.txt')\n",
    "c=0\n",
    "s1=[]\n",
    "s2=[]\n",
    "for line in fh:\n",
    "    line=line.lower()\n",
    "    c+=1\n",
    "    line=line.rstrip()\n",
    "    sentences=line.split('\\t')\n",
    "    s1.append(sentences[0])\n",
    "    s2.append(sentences[1])    \n",
    "    #print(s1,s2)\n",
    "#print(c)\n",
    "\n",
    "def partitioning(sentence):\n",
    "    sen=''\n",
    "    for word in sentence:\n",
    "        for c in string.punctuation:\n",
    "            word=word.replace(c,\"\")\n",
    "        sen+=word\n",
    "    sentence=sen\n",
    "    doc=nlp(sen)\n",
    "    nounp=[]\n",
    "    verbp=[]\n",
    "    nounp=[chunk.text for chunk in doc.noun_chunks]\n",
    "    verbp=[token.lemma_ for token in doc if token.pos_ == \"VERB\"]\n",
    "    return nounp,verbp\n",
    "\n",
    "file=open('resultNP42.txt','w')\n",
    "\n",
    "\n",
    "def listtostr(phrase):\n",
    "    sent=''\n",
    "    for word in phrase:\n",
    "        sent=sent+\" \"+str(word)\n",
    "    return sent\n",
    "\n",
    "ns1=[]\n",
    "ns2=[]\n",
    "i=0\n",
    "print('Length:',len(s1))\n",
    "while(i<len(s1)):\n",
    "    ns1=s1[i]\n",
    "    #print(ns1)\n",
    "    npp,vpp=partitioning(ns1)\n",
    "   # print(npp,vpp)\n",
    "    sen1np=[]\n",
    "    sen1vp=[]\n",
    "    \n",
    "    sen1np=npp\n",
    "    sen1vp=vpp\n",
    "   # print(sen1np, sen1vp) \n",
    "    ns2=s2[i]\n",
    "    #print(ns2)\n",
    "    npp,vpp=partitioning(ns2)\n",
    "   # print(npp,vpp)\n",
    "    sen2np=[]\n",
    "    sen2vp=[]\n",
    "    \n",
    "    sen2np=npp\n",
    "    sen2vp=vpp\n",
    "    \n",
    "   # print(sen2np, sen2vp) \n",
    "    \n",
    "    nppsim=float(bert(sen1np,sen2np))\n",
    "   # print(\"np:\", nppsim)\n",
    "    vppsim=float(bert(sen1vp,sen2vp))\n",
    "   # print(\"vp:\", vppsim)\n",
    "    \n",
    "     \n",
    "    if(nppsim==0 or vppsim==0):\n",
    "        similarity=((nppsim*4)+vppsim)\n",
    "    elif(nppsim !=0 and vppsim !=0):\n",
    "        similarity=(nppsim*4+vppsim)\n",
    "    #print(\"No:\",i+1,similarity)\n",
    "    sim=str(round(similarity,2))+'\\n'\n",
    "    file.writelines(sim)\n",
    "    i+=1\n",
    "    \n",
    "    \n",
    "file.close()\n",
    "\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.stats import spearmanr\n",
    "import numpy as np\n",
    "\n",
    "data1=[]\n",
    "data2=[]\n",
    "f1=open('resultNP42.txt')\n",
    "f2=open('STS.gs.track5.en-en.txt')\n",
    "#f2=open('gs.txt')\n",
    "def listvalue(file):\n",
    "    value=[]\n",
    "    for line in file:\n",
    "        value.append(float(line))\n",
    "    return value\n",
    "data1=listvalue(f1)\n",
    "data2=listvalue(f2)\n",
    "def score(file1, file2):\n",
    "        #mse = round((np.square(np.subtract(file2,file1)).mean()),2)\n",
    "        #print('MSE:',mse)\n",
    "        corr,_=pearsonr(file1,file2)\n",
    "        scor,_=spearmanr(file1,file2)\n",
    "        print('Pearson:',round((corr*100),2),'Spearman:',round((scor*100),2))\n",
    "\n",
    "print(\"bert_role\")\n",
    "score(data1, data2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file=open('new_vector_form_auto.txt','w')\n",
    "ns1=[]\n",
    "ns2=[]\n",
    "i=0\n",
    "print('Length:',len(s1))\n",
    "while(i<250):\n",
    "    ns1=s1[i].split(' ')\n",
    "    sum1=np.zeros((300,), dtype=\"float32\")\n",
    "    w=0\n",
    "    for word in ns1:\n",
    "        for c in string.punctuation:\n",
    "                word=word.replace(c,\"\")\n",
    "        if word not in sw:\n",
    "            if(word in model.wv.vocab):\n",
    "                if w==0:\n",
    "                    sum1=np.add(sum1,model[word])\n",
    "                    w=w+1\n",
    "                else:\n",
    "                    av=sum(sum1)/300\n",
    "                    a=list(model[word])\n",
    "                    sum1=list(sum1)\n",
    "                    m=max(sum1) #extra automation\n",
    "                    k=0\n",
    "                    while(k<300):\n",
    "                        if abs(a[k]-sum1[k])>= (m/8): # works better at 0.135 --> 79.32\n",
    "                            sum1[k]=a[k]+sum1[k]\n",
    "                            k=k+1\n",
    "                        else:\n",
    "                            sum1[k]=sum1[k]\n",
    "                            k=k+1\n",
    "                    sum1=np.asarray(sum1)\n",
    "    average1=np.divide(sum1,1,dtype=\"float32\")\n",
    "            \n",
    "    ns2=s2[i].split(' ')\n",
    "    sum2=np.zeros((300,),dtype=\"float32\")\n",
    "    \n",
    "    w=0\n",
    "    for word in ns2:\n",
    "        for c in string.punctuation:\n",
    "                word=word.replace(c,\"\")\n",
    "        if word not in sw:\n",
    "            if(word in model.wv.vocab):\n",
    "                if w==0:\n",
    "                    sum2=np.add(sum2,model[word])\n",
    "                    w=w+1\n",
    "                else:\n",
    "                    av=sum(sum2)/300\n",
    "                    a=list(model[word])\n",
    "                    sum2=list(sum2)\n",
    "                    m2=max(sum2)\n",
    "                    k=0\n",
    "                    while(k<300):\n",
    "                        if abs(a[k]-sum2[k])>= (m2/8): # works better at 0.135 --> 79.32\n",
    "                            sum2[k]=a[k]+sum2[k]\n",
    "                            k=k+1\n",
    "                        else:\n",
    "                            sum2[k]=sum2[k]\n",
    "                            k=k+1\n",
    "                    sum2=np.asarray(sum2)        \n",
    "    average2=np.divide(sum2,1,dtype=\"float32\")\n",
    "\n",
    "    sim = 1 - spatial.distance.cosine(average1,average2)\n",
    "    sim=sim*5\n",
    "    a=str(round(sim,6))+'\\n'\n",
    "    file.writelines(a)\n",
    "    #print(i,'similarity',sim)\n",
    "    i+=1\n",
    "file.close()\n",
    "\n",
    "data1=[]\n",
    "data2=[]\n",
    "f1=open('new_vector_form_auto.txt')\n",
    "f2=open('STS.gs.track5.en-en.txt')\n",
    "def listvalue(file):\n",
    "    value=[]\n",
    "    for line in file:\n",
    "        value.append(float(line)/5)\n",
    "    return value\n",
    "data1=listvalue(f1)\n",
    "data2=listvalue(f2)\n",
    "corr,_=pearsonr(data1,data2)\n",
    "scor,_=spearmanr(data1,data2)\n",
    "print('FThres_sim:')\n",
    "print('Pearson:',corr*100,'Spearman:',scor*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STS-2017 Dataset is used here.Here, we are taking the first sentence and second sentence into two list. \n",
    "fh=open('SICK1.txt')\n",
    "c=0\n",
    "s1=[]\n",
    "s2=[]\n",
    "score=[]\n",
    "for line in fh:\n",
    "    line=line.lower()\n",
    "    c+=1\n",
    "    line=line.rstrip()\n",
    "    sentences=line.split('\\t')\n",
    "    s1.append(sentences[1])\n",
    "    s2.append(sentences[2])\n",
    "    score.append(sentences[4])\n",
    "print(c)\n",
    "print(len(s1),len(s2),len(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s11=[]\n",
    "s22=[]\n",
    "score1=[]\n",
    "i=0\n",
    "while(i<len(s1)):\n",
    "    if i==0:\n",
    "        print(\"Header\")\n",
    "    else:\n",
    "        s11.append(s1[i])\n",
    "        s22.append(s2[i])\n",
    "        score1.append(score[i])\n",
    "    i=i+1\n",
    "print(len(s11),len(s22),len(score1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file=open('sick_result1.txt','w')\n",
    "ns1=[]\n",
    "ns2=[]\n",
    "i=0\n",
    "print('Length:',len(s11))\n",
    "while(i<len(s11)):\n",
    "    ns1=s11[i].split(' ')\n",
    "    sum1=np.zeros((300,), dtype=\"float32\")\n",
    "    for word in ns1:\n",
    "        for c in string.punctuation:\n",
    "                word=word.replace(c,\"\")\n",
    "        #print(word)\n",
    "        if word not in sw:\n",
    "            if(word in model.wv.vocab): \n",
    "                sum1=np.add(sum1,model[word])\n",
    "    average1=np.divide(sum1,len(ns1),dtype=\"float32\")\n",
    "            \n",
    "    ns2=s22[i].split(' ')\n",
    "    sum2=np.zeros((300,),dtype=\"float32\")\n",
    "    for word in ns2:\n",
    "        for c in string.punctuation:\n",
    "                word=word.replace(c,\"\")\n",
    "        if word not in sw:\n",
    "            if(word in model.wv.vocab):\n",
    "                sum2=np.add(sum2,model[word])\n",
    "    average2=np.divide(sum2,len(ns2),dtype=\"float32\")\n",
    "    sim = 1 - spatial.distance.cosine(average1,average2)\n",
    "    sim=sim*5\n",
    "    a=str(round(sim,6))+'\\n'\n",
    "    file.writelines(a)\n",
    "    #print(i,'similarity',sim)\n",
    "    i+=1 \n",
    "file.close()\n",
    "\n",
    "data1=[]\n",
    "data2=[]\n",
    "f1=open('sick_result1.txt')\n",
    "def listvalue(file):\n",
    "    value=[]\n",
    "    for line in file:\n",
    "        value.append(float(line)/5)\n",
    "    return value\n",
    "data1=listvalue(f1)\n",
    "\n",
    "for i in score1:\n",
    "    data2.append(float(i)/5)\n",
    "print(len(data1),len(data2))\n",
    "corr,_=pearsonr(data1,data2)\n",
    "scor,_=spearmanr(data1,data2)\n",
    "print('Pearson:',corr*100,'Spearman:',scor*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file=open('sick_result_auto_ex.txt','w')\n",
    "d1=[]\n",
    "ns1=[]\n",
    "ns2=[]\n",
    "i=0\n",
    "print('Length:',len(s11))\n",
    "while(i<len(s11)):\n",
    "    ns1=s11[i].split(' ')\n",
    "    sum1=np.zeros((300,), dtype=\"float32\")\n",
    "    for word in ns1:\n",
    "        for c in string.punctuation:\n",
    "            word=word.replace(c,\"\")\n",
    "        if word not in sw:\n",
    "            if(word in model.wv.vocab):\n",
    "                a=model[word]\n",
    "                di=np.std(a)\n",
    "                av=np.mean(a)\n",
    "                k=0\n",
    "                while k<300:\n",
    "                    if a[k]>=(av+di):\n",
    "                    #if abs(sum1[k]-a[k])>=(av-di):\n",
    "                        sum1[k]=sum1[k]+a[k]\n",
    "                        k=k+1\n",
    "                    else:\n",
    "                        k=k+1\n",
    "    average1=np.divide(sum1,1,dtype=\"float32\")\n",
    "         \n",
    "    ns2=s22[i].split(' ')\n",
    "    sum2=np.zeros((300,),dtype=\"float32\")\n",
    "    for word in ns2:\n",
    "        for c in string.punctuation:\n",
    "            word=word.replace(c,\"\")\n",
    "        if word not in sw:\n",
    "            if(word in model.wv.vocab): \n",
    "                a2=model[word]\n",
    "                di2=np.std(a2)\n",
    "                av2=np.mean(a2)\n",
    "                k=0\n",
    "                while k<300:\n",
    "                    if a2[k]>=(av2+di2):\n",
    "                    #if abs(sum2[k]-a2[k])>=(av2-di2):\n",
    "                        sum2[k]=sum2[k]+a2[k]\n",
    "                        k=k+1\n",
    "                    else:\n",
    "                        k=k+1       \n",
    "    average2=np.divide(sum2,1,dtype=\"float32\")\n",
    "\n",
    "    sim = 1 - spatial.distance.cosine(average1,average2)\n",
    "    sim=sim*5\n",
    "    aa=str(round(sim,6))+'\\n'\n",
    "    file.writelines(aa)\n",
    "    d1.append(aa)\n",
    "    i+=1\n",
    "file.close()\n",
    "\n",
    "data1=[]\n",
    "data2=[]\n",
    "file=open('sick_result_auto_ex.txt')\n",
    "\n",
    "def listvalue(file):\n",
    "    value=[]\n",
    "    for line in file:\n",
    "        value.append(float(line)/5)\n",
    "    return value\n",
    "data1=listvalue(file)\n",
    "\n",
    "for i in score1:\n",
    "    data2.append(float(i)/5)\n",
    "print(len(data1),len(data2))\n",
    "corr,_=pearsonr(data1,data2)\n",
    "scor,_=spearmanr(data1,data2)\n",
    "print('Pearson:',corr*100,'Spearman:',scor*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
