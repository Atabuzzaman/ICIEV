{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For cross-lingual texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For English Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Required Libraries and package of python\n",
    "import nltk\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import KeyedVectors\n",
    "from nltk.tokenize import sent_tokenize,word_tokenize\n",
    "import numpy as np\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.stats import spearmanr\n",
    "from scipy import spatial\n",
    "import spacy\n",
    "nlp=spacy.load('en_core_web_sm')\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "import nltk \n",
    "from nltk.tokenize import LineTokenizer \n",
    "from nltk.stem import PorterStemmer     \n",
    "tk = LineTokenizer()   \n",
    "lm = WordNetLemmatizer() \n",
    "from nltk.corpus import wordnet as wn\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pretrained Word2Vec model. It was pretrained on Google News Corpus\n",
    "filename = 'GoogleNews-vectors-negative300.bin'\n",
    "model = KeyedVectors.load_word2vec_format(filename, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "sw=stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STS-2017 Dataset is used here.Here, we are taking the first sentence and second sentence into two list. \n",
    "fh=open('STS.input.track51.en-en.txt')\n",
    "c=0\n",
    "s1=[]\n",
    "s2=[]\n",
    "for line in fh:\n",
    "    line=line.lower()\n",
    "    c+=1\n",
    "    line=line.rstrip()\n",
    "    sentences=line.split('\\t')\n",
    "    s1.append(sentences[0])\n",
    "    s2.append(sentences[1]) \n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maximum word-level Similarity(MWL_S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file=open('resultwordcommean1.txt','w')\n",
    "ns1=[]\n",
    "ns2=[]\n",
    "i=0\n",
    "print('Length:',len(s1))\n",
    "while(i<len(s1)):\n",
    "    ns1=s1[i].split(' ')\n",
    "    s1word=[]\n",
    "    for word in ns1:\n",
    "        for c in string.punctuation:\n",
    "            word=word.replace(c,'')\n",
    "        if word not in sw:\n",
    "            s1word.append(lemmatizer.lemmatize(word))\n",
    "    #print(s1word)\n",
    "    s2word=[]\n",
    "    ns2=s2[i].split(' ')\n",
    "    for word in ns2:\n",
    "        for c in string.punctuation:\n",
    "                word=word.replace(c,\"\")\n",
    "        if word not in sw:\n",
    "            s2word.append(lemmatizer.lemmatize(word))\n",
    "    #print(s2word)\n",
    "    max_score=[]\n",
    "    for word in s1word:\n",
    "        if word in model.wv.vocab:\n",
    "            score=[]\n",
    "            for w in s2word:\n",
    "                if w in model.wv.vocab:\n",
    "                    score.append(model.similarity(word,w))\n",
    "        max_score.append(max(score))\n",
    "    #print(max_score)\n",
    "    \n",
    "    max_score2=[]\n",
    "    for word in s2word:\n",
    "        if word in model.wv.vocab:\n",
    "            score=[]\n",
    "            for w in s1word:\n",
    "                if w in model.wv.vocab:\n",
    "                    score.append(model.similarity(word,w))\n",
    "        max_score2.append(max(score))\n",
    "    #print(max_score2)\n",
    "    \n",
    "    count=0\n",
    "    summ=0\n",
    "    for val in max_score:\n",
    "        summ+=val\n",
    "        count+=1\n",
    "    average=(summ*5)/count\n",
    "    \n",
    "    count1=0\n",
    "    summ1=0\n",
    "    for val in max_score2:\n",
    "        summ1+=val\n",
    "        count1+=1\n",
    "    average1=(summ1*5)/count1\n",
    "    \n",
    "    aver=(average+average1)/2\n",
    "    #print('i:',i+1,aver)\n",
    "    a=str(round(aver,6))+'\\n'\n",
    "    file.writelines(a)\n",
    "    i+=1\n",
    "    \n",
    "file.close()\n",
    "data1=[]\n",
    "data2=[]\n",
    "f1=open('resultwordcommean1.txt')\n",
    "f2=open('STS.gs.track5.en-en.txt')\n",
    "def listvalue(file):\n",
    "    value=[]\n",
    "    for line in file:\n",
    "        value.append(float(line)/5)\n",
    "    return value\n",
    "data1=listvalue(f1)\n",
    "data2=listvalue(f2)\n",
    "#print(len(data1),len(data2))\n",
    "#mse = np.square(np.subtract(data2,data1)).mean()\n",
    "#print('MSE:',mse)\n",
    "corr,_=pearsonr(data1,data2)\n",
    "scor,_=spearmanr(data1,data2)\n",
    "print('Pearson:',corr*100,'Spearman:',scor*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Average Feature Vector based Similarity(V_avgS):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file=open('result1.txt','w')\n",
    "ns1=[]\n",
    "ns2=[]\n",
    "i=0\n",
    "print('Length:',len(s1))\n",
    "while(i<len(s1)):\n",
    "    ns1=s1[i].split(' ')\n",
    "    sum1=np.zeros((300,), dtype=\"float32\")\n",
    "    for word in ns1:\n",
    "        for c in string.punctuation:\n",
    "                word=word.replace(c,\"\")\n",
    "        #print(word)\n",
    "        if word not in sw:\n",
    "            if(word in model.wv.vocab): \n",
    "                sum1=np.add(sum1,model[word])\n",
    "    average1=np.divide(sum1,len(ns1),dtype=\"float32\")\n",
    "            \n",
    "    ns2=s2[i].split(' ')\n",
    "    sum2=np.zeros((300,),dtype=\"float32\")\n",
    "    for word in ns2:\n",
    "        for c in string.punctuation:\n",
    "                word=word.replace(c,\"\")\n",
    "        if word not in sw:\n",
    "            if(word in model.wv.vocab):\n",
    "                sum2=np.add(sum2,model[word])\n",
    "    average2=np.divide(sum2,len(ns2),dtype=\"float32\")\n",
    "    sim = 1 - spatial.distance.cosine(average1,average2)\n",
    "    sim=sim*5\n",
    "    a=str(round(sim,6))+'\\n'\n",
    "    file.writelines(a)\n",
    "    #print(i,'similarity',sim)\n",
    "    i+=1 \n",
    "file.close()\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "data1=[]\n",
    "data2=[]\n",
    "f1=open('result1.txt')\n",
    "f2=open('STS.gs.track5.en-en.txt')\n",
    "def listvalue(file):\n",
    "    value=[]\n",
    "    for line in file:\n",
    "        value.append(float(line)/5)\n",
    "    return value\n",
    "data1=listvalue(f1)\n",
    "data2=listvalue(f2)\n",
    "#print(len(data1),len(data2))\n",
    "#mse = np.square(np.subtract(data2,data1)).mean()\n",
    "#print('MSE:',mse)\n",
    "#ms=mean_squared_error(data2,data1)\n",
    "#print('MS:',ms)\n",
    "corr,_=pearsonr(data1,data2)\n",
    "scor,_=spearmanr(data1,data2)\n",
    "print('Pearson:',corr*100,'Spearman:',scor*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maximum word-level similarity of NP, VP (MWLR_Sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file=open('resultNP_VP_compare12.txt','w')\n",
    "\n",
    "def partitioning(sentence):\n",
    "    sen=''\n",
    "    for word in sentence:\n",
    "        for c in string.punctuation:\n",
    "            word=word.replace(c,\"\")\n",
    "        sen+=word\n",
    "    sentence=sen\n",
    "    doc=nlp(sen)\n",
    "    nounp=[]\n",
    "    verbp=[]\n",
    "    nounp=[chunk.text for chunk in doc.noun_chunks]\n",
    "    verbp=[token.lemma_ for token in doc if token.pos_ == \"VERB\"]\n",
    "    return nounp,verbp\n",
    "\n",
    "def listtostr(phrase):\n",
    "    sent=''\n",
    "    for word in phrase:\n",
    "        sent=sent+\" \"+str(word.strip())\n",
    "    return sent\n",
    "\n",
    "def result(s1,s2):\n",
    "    s1word=[]\n",
    "    s2word=[]\n",
    "    for word in s1:\n",
    "        if word not in sw:\n",
    "            s1word.append(word)\n",
    "    for word in s2:\n",
    "        if word not in sw:\n",
    "            s2word.append(word)\n",
    "            \n",
    "    #print(s1word,s2word)\n",
    "    if len(s1word)==0 or len(s2word)==0:\n",
    "        aver=0\n",
    "    else:\n",
    "        max_score=[]\n",
    "        for word in s1word:\n",
    "            score=[0]\n",
    "            if word in model.wv.vocab:\n",
    "                for w in s2word:\n",
    "                    if w in model.wv.vocab:\n",
    "                        score.append(model.similarity(word,w))\n",
    "            max_score.append(max(score))\n",
    "        #print(max_score)\n",
    "    \n",
    "        max_score2=[]\n",
    "        for word in s2word:\n",
    "            score=[0]\n",
    "            if word in model.wv.vocab:\n",
    "                for w in s1word:\n",
    "                    if w in model.wv.vocab:\n",
    "                        score.append(model.similarity(word,w))\n",
    "            max_score2.append(max(score))\n",
    "        #print(max_score2)\n",
    "    \n",
    "        average=sum(max_score)/(len(max_score))\n",
    "    \n",
    "        average1=sum(max_score2)/(len(max_score2))\n",
    "    \n",
    "        aver=(average+average1)/2\n",
    "    return aver\n",
    "\n",
    "ns1=[]\n",
    "ns2=[]\n",
    "i=0\n",
    "print('Length:',len(s1))\n",
    "while(i<len(s1)):\n",
    "    ns1=s1[i]\n",
    "    #print(ns1)\n",
    "    npp,vpp=partitioning(ns1)\n",
    "    #print(npp,vpp)\n",
    "    sen1np=listtostr(npp).strip()\n",
    "    sen1np=sen1np.split(' ')\n",
    "    #print(sen1np, len(sen1np))\n",
    "    sen1vp=listtostr(vpp).strip()\n",
    "    sen1vp=sen1vp.split(' ')\n",
    "        \n",
    "    ns2=s2[i]\n",
    "    #print(ns2)\n",
    "    npp,vpp=partitioning(ns2)\n",
    "    #print(npp,vpp)\n",
    "    sen2np=listtostr(npp).strip()\n",
    "    sen2np=sen2np.split(' ')\n",
    "    sen2vp=listtostr(vpp).strip()\n",
    "    sen2vp=sen2vp.split(' ')\n",
    "    \n",
    "    avernp=result(sen1np,sen2np)\n",
    "    #print('NP:',avernp)\n",
    "    avervp=result(sen1vp,sen2vp)\n",
    "    #print('VP:',avervp)\n",
    "    #if(avernp==0 or avervp==0):\n",
    "     #   av=(avernp+avervp)*5\n",
    "    #else:\n",
    "    av=avernp*4+avervp\n",
    "    av=str(round(av,6))+'\\n'\n",
    "    #print('No:',i+1,'similarity:',av)\n",
    "    file.writelines(av)\n",
    "    \n",
    "    i=i+1\n",
    "    \n",
    "file.close()\n",
    "\n",
    "data1=[]\n",
    "data2=[]\n",
    "f1=open('resultNP_VP_compare12.txt')\n",
    "f2=open('STS.gs.track5.en-en.txt')\n",
    "def listvalue(file):\n",
    "    value=[]\n",
    "    for line in file:\n",
    "        value.append(float(line)/5)\n",
    "    return value\n",
    "data1=listvalue(f1)\n",
    "data2=listvalue(f2)\n",
    "print(len(data1),len(data2))\n",
    "mse=np.square(np.subtract(data2,data1)).mean()\n",
    "print(\"MSE:\",mse)\n",
    "corr,_=pearsonr(data1,data2)\n",
    "scor,_=spearmanr(data1,data2)\n",
    "print('Pearson:',corr*100,'Spearman:',scor*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parts-of-Speech(POS) tagged-based similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file=open('resultpos1.txt','w')\n",
    "\n",
    "ns1=[]\n",
    "ns2=[]\n",
    "i=0\n",
    "print('Length:',len(s1))\n",
    "while(i<len(s1)):\n",
    "    ns1=s1[i].split(' ')\n",
    "    s1word=[]\n",
    "    for word in ns1:\n",
    "        for c in string.punctuation:\n",
    "            word=word.replace(c,'')\n",
    "        if word not in sw:\n",
    "            s1word.append(word)\n",
    "    #print(s1word)\n",
    "    s2word=[]\n",
    "    ns2=s2[i].split(' ')\n",
    "    for word in ns2:\n",
    "        for c in string.punctuation:\n",
    "                word=word.replace(c,\"\")\n",
    "        if word not in sw:\n",
    "            s2word.append(word)\n",
    "    #print(s2word)\n",
    "    tag1=nltk.pos_tag(s1word,tagset='universal')\n",
    "    tag2=nltk.pos_tag(s2word,tagset='universal')\n",
    "    #print(tag1,tag2)\n",
    "    k=0\n",
    "    max_score=[]\n",
    "    for k in tag1:\n",
    "        sscore=[]\n",
    "        for j in tag2:\n",
    "            if (k[1]==j[1]):\n",
    "                if k[0] in model.wv.vocab:\n",
    "                    if j[0] in model.wv.vocab:\n",
    "                        a=model.similarity(k[0],j[0])\n",
    "                        #print(k[0],j[0],a)\n",
    "                        sscore.append(a)\n",
    "                    #print(sscore)\n",
    "        if(len(sscore)>=1):\n",
    "            max_score.append(max(sscore))\n",
    "    #print(max_score)\n",
    "    count=0\n",
    "    summ=0\n",
    "    for val in max_score:\n",
    "        summ+=val\n",
    "        count+=1\n",
    "    average=(summ*5)/count\n",
    "    #print('i:',i+1,average)\n",
    "    a=str(round(average,6))+'\\n'\n",
    "    file.writelines(a)\n",
    "    i+=1\n",
    "file.close()\n",
    "\n",
    "data1=[]\n",
    "data2=[]\n",
    "f1=open('resultpos1.txt')\n",
    "f2=open('STS.gs.track5.en-en.txt')\n",
    "def listvalue(file):\n",
    "    value=[]\n",
    "    for line in file:\n",
    "        value.append(float(line)/5)\n",
    "    return value\n",
    "data1=listvalue(f1)\n",
    "data2=listvalue(f2)\n",
    "print(len(data1),len(data2))\n",
    "mse = np.square(np.subtract(data2,data1)).mean()\n",
    "print('MSE:',mse)\n",
    "corr,_=pearsonr(data1,data2)\n",
    "scor,_=spearmanr(data1,data2)\n",
    "print('Pearson:',corr*100,'Spearman:',scor*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For Bengali texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import gensim\n",
    "from gensim.models import KeyedVectors\n",
    "from bnlp import BengaliWord2Vec\n",
    "bwv = BengaliWord2Vec()\n",
    "from bnlp import NLTKTokenizer\n",
    "bnltk = NLTKTokenizer()\n",
    "from bnlp import NER\n",
    "bn_ner = NER()\n",
    "from bnlp import POS\n",
    "bn_pos = POS()\n",
    "import fasttext\n",
    "import numpy as np\n",
    "from scipy import spatial\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.stats import spearmanr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.wrappers import FastText\n",
    "model=FastText.load_fasttext_format('wiki.bn.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STS-2017 Dataset is used here.Here, we are taking the first sentence and second sentence into two list. \n",
    "fh=open('STS-Bengali.txt',encoding = 'utf-8')\n",
    "c=0\n",
    "s1=[]\n",
    "s2=[]\n",
    "for line in fh:\n",
    "    line=line.lower()\n",
    "    c+=1\n",
    "    line=line.rstrip()\n",
    "    sentences=line.split('$')\n",
    "    s1.append(sentences[0])\n",
    "    s2.append(sentences[1]) \n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation=['ред', ',', ':', ';', '-', '!','?']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "s11=[]\n",
    "while i<len(s1):\n",
    "    wd=bnltk.word_tokenize(s1[i])\n",
    "    for word in wd:\n",
    "        if word in punctuation:\n",
    "            wd.remove(word)\n",
    "    i=i+1\n",
    "    s11.append(wd)\n",
    "\n",
    "i=0\n",
    "s22=[]\n",
    "while i<len(s1):\n",
    "    wd=bnltk.word_tokenize(s2[i])\n",
    "    for word in wd:\n",
    "        if word in punctuation:\n",
    "            wd.remove(word)\n",
    "    i=i+1\n",
    "    s22.append(wd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bangla_stemmer.stemmer import stemmer\n",
    "stmr = stemmer.BanglaStemmer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maximum word-level Similarity(MWL_S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file=open('Bengali_result_stemmer.txt','w')\n",
    "i=0\n",
    "similarity2=[]\n",
    "while i<250:\n",
    "    s11[i]=stmr.stem(s11[i])\n",
    "    s22[i]=stmr.stem(s22[i])\n",
    "    #print(s11[i])\n",
    "    #print(s22[i])\n",
    "    max_score=[]\n",
    "    for word in s11[i]:\n",
    "        if word in model.wv.vocab:\n",
    "            score=[]\n",
    "            for w in s22[i]:\n",
    "                if w in model.wv.vocab:\n",
    "                    score.append(model.similarity(word,w))\n",
    "        max_score.append(max(score))\n",
    "    #print(max_score)\n",
    "    \n",
    "    max_score2=[]\n",
    "    for word in s22[i]:\n",
    "        if word in model.wv.vocab:\n",
    "            score=[]\n",
    "            for w in s11[i]:\n",
    "                if w in model.wv.vocab:\n",
    "                    score.append(model.similarity(word,w))\n",
    "        max_score2.append(max(score))\n",
    "    #print(max_score2)\n",
    "    \n",
    "    count=0\n",
    "    summ=0\n",
    "    a=max(max_score)\n",
    "    for val in max_score:\n",
    "        if(val >= a/2):\n",
    "            summ+=val\n",
    "            count+=1\n",
    "    average=(summ*5)/len(max_score)\n",
    "    \n",
    "    count1=0\n",
    "    summ1=0\n",
    "    b=max(max_score2)\n",
    "    for val in max_score2:\n",
    "        if(val>= b/2):\n",
    "            summ1+=val\n",
    "            count1+=1\n",
    "    average1=(summ1*5)/len(max_score2)\n",
    "    \n",
    "    aver=(average+average1)/2\n",
    "    #similarity4.append(aver)\n",
    "    a=str(round(aver,6))+'\\n'\n",
    "    file.writelines(a)\n",
    "    similarity2.append(aver)\n",
    "    i=i+1\n",
    "    #print('NO:',i,\"sim:\",aver)\n",
    "file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1=[]\n",
    "data2=[]\n",
    "f1=open('Bengali_result_stemmer.txt')\n",
    "f2=open('Gold-Standard.txt')\n",
    "def listvalue(file):\n",
    "    value=[]\n",
    "    for line in file:\n",
    "        value.append(float(line))\n",
    "    return value\n",
    "data1=listvalue(f1)\n",
    "data2=listvalue(f2)\n",
    "print(len(data1),len(data2))\n",
    "corr,_=pearsonr(data1,data2)\n",
    "scor,_=spearmanr(data1,data2)\n",
    "print('Pearson:',corr*100,'Spearman:',scor*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BOW approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For Bag of words and tf-idf\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv=CountVectorizer()\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer=TfidfVectorizer(norm=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1=open('bengali_bow.txt','w')\n",
    "\n",
    "def vectorize(tokens):\n",
    "    vector=[]\n",
    "    for w in vocab:\n",
    "        vector.append(tokens.count(w))\n",
    "    return vector\n",
    "\n",
    "def unique(sequence):\n",
    "    seen = set()\n",
    "    return [x for x in sequence if not (x in seen or seen.add(x))]\n",
    "\n",
    "i=0\n",
    "while i<250:\n",
    "    vocab=unique(s11[i]+s22[i])\n",
    "    \n",
    "    vector1=vectorize(s11[i])\n",
    "    \n",
    "    vector2=vectorize(s22[i])\n",
    "    \n",
    "    sim = 1 - spatial.distance.cosine(vector1,vector2)\n",
    "    \n",
    "    a=str(round(sim,2))+'\\n'\n",
    "    f1.writelines(a)\n",
    "    \n",
    "    i=i+1\n",
    "f1.close() \n",
    "\n",
    "data1=[]\n",
    "data2=[]\n",
    "f1=open('Bengali_bow.txt')\n",
    "f2=open('Gold-Standard.txt')\n",
    "def listvalue(file):\n",
    "    value=[]\n",
    "    for line in file:\n",
    "        value.append(float(line))\n",
    "    return value\n",
    "data1=listvalue(f1)\n",
    "data2=listvalue(f2)\n",
    "#print(len(data1),len(data2))\n",
    "corr,_=pearsonr(data1,data2)\n",
    "scor,_=spearmanr(data1,data2)\n",
    "print('Pearson:',corr*100,'Spearman:',scor*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maximum word-level similarity of NP, VP (MWL_NVP_S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bnlp import POS\n",
    "bn_pos = POS()\n",
    "model_path = \"bn_pos.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity3=[]\n",
    "i=0\n",
    "while i<250:\n",
    "    n1=[]\n",
    "    p1=[]\n",
    "    aj1=[]\n",
    "    v1=[]\n",
    "    \n",
    "    n2=[]\n",
    "    p2=[]\n",
    "    aj2=[]\n",
    "    v2=[]\n",
    "    res = bn_pos.tag(model_path, s1[i])\n",
    "    res2=bn_pos.tag(model_path, s2[i])\n",
    "    \n",
    "    for word in res:\n",
    "        if word[1]=='VM':\n",
    "            v1.append(word[0])\n",
    "        elif word[1]=='NC' or word[1]=='NP':\n",
    "            n1.append(word[0])\n",
    "        elif word[1]=='PPR' or word[1]=='PRF':\n",
    "            p1.append(word[0])\n",
    "        elif word[1]=='JJ' or word[1]=='JQ':\n",
    "            #aj1.append(word[0])\n",
    "            n1.append(word[0])\n",
    "    \n",
    "    for word in res2:\n",
    "        if word[1]=='VM':\n",
    "            v2.append(word[0])\n",
    "        elif word[1]=='NC' or word[1]=='NP':\n",
    "            n2.append(word[0])\n",
    "        elif word[1]=='PPR' or word[1]=='PRF':\n",
    "            p2.append(word[0])\n",
    "        elif word[1]=='JJ' or word[1]=='JQ':\n",
    "            #aj2.append(word[0])\n",
    "            n2.append(word[0])\n",
    "            \n",
    "    def get_sim(p1,p2):\n",
    "        p1=stmr.stem(p1)\n",
    "        p2=stmr.stem(p2)\n",
    "        \n",
    "        max_score=[]\n",
    "        max_score2=[]\n",
    "        \n",
    "        for word in p1:\n",
    "            score=[]\n",
    "            if word in model.wv.vocab:\n",
    "                \n",
    "                for w in p2:\n",
    "                    if w in model.wv.vocab:\n",
    "                        score.append(model.similarity(word,w))\n",
    "            if len(score)!=0:\n",
    "                max_score.append(max(score))\n",
    "    #print(max_score)\n",
    "    \n",
    "        max_score2=[]\n",
    "        for word in p2:\n",
    "            score=[]\n",
    "            if word in model.wv.vocab:\n",
    "                \n",
    "                for w in p1:\n",
    "                    if w in model.wv.vocab:\n",
    "                        score.append(model.similarity(word,w))\n",
    "                \n",
    "            if len(score)!=0:\n",
    "                max_score2.append(max(score))\n",
    "        if len(max_score)!=0:\n",
    "            sim=sum(max_score)/len(max_score)\n",
    "        else:\n",
    "            sim=0\n",
    "        if len(max_score2)!=0:\n",
    "            sim2=sum(max_score)/len(max_score2)\n",
    "        else:\n",
    "            sim2=0\n",
    "        similar=(sim+sim2)/2\n",
    "    \n",
    "        return similar\n",
    "    \n",
    "    nsim=get_sim(n1,n2)\n",
    "    vsim=get_sim(v1,v2)\n",
    "    ajsim=get_sim(aj1,aj2)\n",
    "    psim=get_sim(p1,p2)\n",
    "    \n",
    "    sim_score=nsim*4.25 +0.75* vsim\n",
    "    similarity3.append(sim_score)\n",
    "    i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1=[]\n",
    "data2=[]\n",
    "\n",
    "f2=open('Gold-Standard.txt')\n",
    "def listvalue(file):\n",
    "    value=[]\n",
    "    for line in file:\n",
    "        value.append(float(line))\n",
    "    return value\n",
    "\n",
    "data2=listvalue(f2)\n",
    "print(len(similarity3),len(data2))\n",
    "corr,_=pearsonr(similarity3,data2)\n",
    "scor,_=spearmanr(similarity3,data2)\n",
    "print('Pearson:',corr*100,'Spearman:',scor*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parts-of-Speech(POS) tagged-based similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation=['ред', ',', ':', ';', '-', '!','?']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity=[]\n",
    "i=0\n",
    "while i<250:\n",
    "    text = []\n",
    "    text2= []\n",
    "    res = bn_pos.tag(model_path, s1[i])\n",
    "    res2=bn_pos.tag(model_path, s2[i])\n",
    "    k=0\n",
    "    while(k<len(res)):\n",
    "        word=res[k][0]\n",
    "        if word in punctuation:\n",
    "            k=k+1\n",
    "        else:\n",
    "            word=stmr.stem(word)\n",
    "            score=[]\n",
    "            if word in model.wv.vocab:\n",
    "            #score=[]\n",
    "                j=0\n",
    "                while(j<len(res2)):\n",
    "                    if(res[k][1]==res2[j][1]):\n",
    "                        word2=res2[j][0]\n",
    "                        word2=stmr.stem(word2)\n",
    "                        if word2 in model.wv.vocab:\n",
    "                            #print(word, word2)\n",
    "                            score.append(model.similarity(word,word2))\n",
    "                    j=j+1\n",
    "                if len(score)!=0:\n",
    "                    text.append(max(score))\n",
    "            k=k+1\n",
    "    if len(text)!=0:\n",
    "        sim=sum(text)/len(text)\n",
    "    #print(text)\n",
    "    #print(\"similarity:\",sim*5)\n",
    "    \n",
    "    k=0\n",
    "    while(k<len(res2)):\n",
    "        word=res2[k][0]\n",
    "        if word in punctuation:\n",
    "            k=k+1\n",
    "        else:\n",
    "            word=stmr.stem(word)\n",
    "            score2=[]\n",
    "            if word in model.wv.vocab:\n",
    "                #score2=[]\n",
    "                j=0\n",
    "                while(j<len(res)):\n",
    "                    if(res2[k][1]==res[j][1]):\n",
    "                        word2=res[j][0]\n",
    "                        word2=stmr.stem(word2)\n",
    "                        if word2 in model.wv.vocab:\n",
    "                            #print(word, word2)\n",
    "                            score2.append(model.similarity(word,word2))\n",
    "                    j=j+1\n",
    "                if len(score2)!=0:\n",
    "                    text2.append(max(score2))\n",
    "            k=k+1\n",
    "    if len(text2)!=0:\n",
    "        sim2=sum(text2)/len(text2)\n",
    "    #print(text2)\n",
    "    #print(sim2*5)\n",
    "    average=(sim+sim2)/2\n",
    "    \n",
    "    similarity.append(average*5)\n",
    "    print('no:',i+1, average*5)\n",
    "    i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1=[]\n",
    "data2=[]\n",
    "#f1=open('result1.txt')\n",
    "f2=open('Gold-Standard.txt')\n",
    "def listvalue(file):\n",
    "    value=[]\n",
    "    for line in file:\n",
    "        value.append(float(line))\n",
    "    return value\n",
    "#data1=listvalue(f1)\n",
    "data2=listvalue(f2)\n",
    "print(len(similarity),len(data2))\n",
    "corr,_=pearsonr(similarity,data2)\n",
    "scor,_=spearmanr(similarity,data2)\n",
    "print('Pearson:',corr*100,'Spearman:',scor*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Average Feature Vector based Similarity(V_avgS):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#file=open('result1.txt','w')\n",
    "ns1=[]\n",
    "ns2=[]\n",
    "i=0\n",
    "similarity2=[]\n",
    "similarity3=[]\n",
    "print('Length:',len(s1))\n",
    "\n",
    "while(i<len(s11)):\n",
    "    #print(s11[i])\n",
    "    sum1=np.zeros((300,), dtype=\"float32\")\n",
    "    c=0\n",
    "    for word in s11[i]:\n",
    "        if(word in model.wv.vocab):\n",
    "            c=c+1\n",
    "            #print(word)\n",
    "            sum1=np.add(sum1,model[word])\n",
    "    #average1=np.divide(sum1,len(s11[i]),dtype=\"float32\")\n",
    "    average1=np.divide(sum1,c,dtype=\"float32\")\n",
    "   \n",
    "    #print(s22[i])\n",
    "    sum2=np.zeros((300,),dtype=\"float32\")\n",
    "    c=0\n",
    "    for word in s22[i]:\n",
    "        if(word in model.wv.vocab):\n",
    "            c=c+1\n",
    "            #print(word)\n",
    "            sum2=np.add(sum2,model[word])\n",
    "    average2=np.divide(sum2,c,dtype=\"float32\")\n",
    "    \n",
    "    sim2 = 1 - spatial.distance.cosine(sum1,sum2)\n",
    "    sim2=sim2*5\n",
    "    similarity3.append(sim2)\n",
    "    sim = 1 - spatial.distance.cosine(average1,average2)\n",
    "    sim=sim*5\n",
    "    \n",
    "    similarity2.append(sim)\n",
    "    #print(i, sim)\n",
    "    #a=str(round(sim,6))+'\\n'\n",
    "    #file.writelines(a)\n",
    "    #print(i,'similarity',sim)\n",
    "    i+=1 \n",
    "#file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data1=[]\n",
    "data2=[]\n",
    "#f1=open('result1.txt')\n",
    "f2=open('Gold-Standard.txt')\n",
    "def listvalue(file):\n",
    "    value=[]\n",
    "    for line in file:\n",
    "        value.append(float(line)/5)\n",
    "    return value\n",
    "#data1=listvalue(f1)\n",
    "data2=listvalue(f2)\n",
    "#print(len(similarity2),len(data2))\n",
    "print('average\\n')\n",
    "corr,_=pearsonr(similarity2,data2)\n",
    "scor,_=spearmanr(similarity2,data2)\n",
    "print('Pearson:',corr*100,'Spearman:',scor*100)\n",
    "#print('sum\\n')\n",
    "#print(len(similarity3),len(data2))\n",
    "corr,_=pearsonr(similarity3,data2)\n",
    "scor,_=spearmanr(similarity3,data2)\n",
    "#print('Pearson:',corr*100,'Spearman:',scor*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monolingual Texts (English)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import KeyedVectors\n",
    "from nltk.tokenize import sent_tokenize,word_tokenize\n",
    "import numpy as np\n",
    "from scipy import spatial\n",
    "\n",
    "filename = 'GoogleNews-vectors-negative300.bin'\n",
    "model = KeyedVectors.load_word2vec_format(filename, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fh=open('STS.input.track5.en-en.txt')\n",
    "c=0\n",
    "s1=[]\n",
    "s2=[]\n",
    "for line in fh:\n",
    "    line=line.lower()\n",
    "    c+=1\n",
    "    line=line.rstrip()\n",
    "    sentences=sent_tokenize(line)\n",
    "    s1.append(sentences[0])\n",
    "    s2.append(sentences[1])    \n",
    "#print(s1,s2)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maximum-Role level similarity(RM_sim):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file=open('newNpVp.txt','w')\n",
    "\n",
    "def partitioning(sentence):\n",
    "    sen=''\n",
    "    for word in sentence:\n",
    "        for c in string.punctuation:\n",
    "            word=word.replace(c,\"\")\n",
    "        sen+=word\n",
    "    sentence=sen\n",
    "    doc=nlp(sen)\n",
    "    nounp=[]\n",
    "    verbp=[]\n",
    "    nounp=[chunk.text for chunk in doc.noun_chunks]\n",
    "    verbp=[token.lemma_ for token in doc if token.pos_ == \"VERB\"]\n",
    "    return nounp,verbp\n",
    "\n",
    "def vectorvalue(subsent):\n",
    "    #print(subsent)\n",
    "    subsent=subsent.split(\" \")\n",
    "    sum1=np.zeros((300,), dtype=\"float32\")\n",
    "    for word in subsent:\n",
    "        if word not in sw:\n",
    "            if(word in model.wv.vocab):\n",
    "                sum1=np.add(sum1,model[word])\n",
    "    if len(subsent)>0:\n",
    "        average1=np.divide(sum1,len(subsent),dtype=\"float32\")\n",
    "    else:\n",
    "        average1=sum1\n",
    "    return average1\n",
    "\n",
    "def listvalue(pp):\n",
    "    pplist=[]\n",
    "    if(len(pp)>0):\n",
    "        j=0\n",
    "        while(j<len(pp)):\n",
    "            pplist.append(vectorvalue(pp[j]))\n",
    "            j=j+1\n",
    "    else:\n",
    "        pplist.append(np.zeros((300,), dtype=\"float32\"))\n",
    "    return pplist\n",
    "\n",
    "def score(nplist,np2list,vplist,vp2list,a):\n",
    "    simlist=[]\n",
    "    if(np.all(nplist)==0 and np.all(np2list)==0 and np.all(vplist)==0 and np.all(vp2list)==0):\n",
    "        aa=0\n",
    "    else:\n",
    "        for value in nplist:\n",
    "            npsim=[]\n",
    "            for value2 in np2list:\n",
    "                sim=(1 - spatial.distance.cosine(value,value2))*(5/a)\n",
    "                if( np.isnan(sim)==1):\n",
    "                    sim=0\n",
    "                npsim.append(sim)\n",
    "            #print('np:',max(npsim))\n",
    "            simlist.append(max(npsim))\n",
    "        \n",
    "        for v in vplist:\n",
    "            vpsim=[]\n",
    "            for v2 in vp2list:\n",
    "                sim=(1-spatial.distance.cosine(v,v2))*(5/a)\n",
    "                if np.isnan(sim)==1:\n",
    "                    sim=0\n",
    "                vpsim.append(sim)\n",
    "            #print('vp:',max(vpsim))\n",
    "            simlist.append(max(vpsim))\n",
    "        if(len(simlist)>0):    \n",
    "            bb=max(simlist)\n",
    "            p=0\n",
    "            while(p<len(simlist)):\n",
    "                if simlist[p]<bb/2:\n",
    "                    simlist[p]=0\n",
    "                p=p+1\n",
    "        aa = sum(simlist)\n",
    "    return aa\n",
    "\n",
    "ns1=[]\n",
    "ns2=[]\n",
    "i=0\n",
    "print('Length:',len(s1))\n",
    "while(i<len(s1)):\n",
    "    ns1=s1[i]\n",
    "    #print(ns1)\n",
    "    nplist=[]\n",
    "    vplist=[]\n",
    "    np2list=[]\n",
    "    vp2list=[]\n",
    "    npp,vpp=partitioning(ns1)\n",
    "    #print(npp,vpp)\n",
    "    nplist=listvalue(npp)\n",
    "    vplist=listvalue(vpp)\n",
    "    a=len(npp)+len(vpp)\n",
    "    \n",
    "    ns2=s2[i]\n",
    "    #print(ns2)\n",
    "    npp2,vpp2=partitioning(ns2)\n",
    "    #print(npp2,vpp2)\n",
    "    np2list=listvalue(npp2)\n",
    "    vp2list=listvalue(vpp2)\n",
    "    b=len(npp2)+len(vpp2)\n",
    "    \n",
    "    if a>=b:\n",
    "        sim=score(nplist,np2list,vplist,vp2list,a)\n",
    "    else:\n",
    "        sim=score(np2list,nplist,vp2list,vplist,b)\n",
    "    #print(i+1,'similarity:',sim)\n",
    "    sim=str(round(sim,2))+'\\n'\n",
    "    file.writelines(sim)\n",
    "    i=i+1\n",
    "    \n",
    "file.close()\n",
    "\n",
    "data1=[]\n",
    "data2=[]\n",
    "f2=open('STS.gs.track5.en-en.txt')\n",
    "f1=open('newNpVp.txt')\n",
    "#f2=open('newNpVp.txt')\n",
    "\n",
    "def listvalue(file):\n",
    "    value=[]\n",
    "    for line in file:\n",
    "        value.append(float(line)/5)\n",
    "    return value\n",
    "data1=listvalue(f1)\n",
    "data2=listvalue(f2)\n",
    "print(len(data1),len(data2))\n",
    "mse = np.square(np.subtract(data2,data1)).mean()\n",
    "#print('MSE:',mse)\n",
    "corr,_=pearsonr(data1,data2)\n",
    "scor,_=spearmanr(data1,data2)\n",
    "print('Pearson:',corr*100,'Spearman:',scor*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Threshold based similiarity (Fthre_S):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file=open('new_vector_form.txt','w')\n",
    "ns1=[]\n",
    "ns2=[]\n",
    "i=0\n",
    "print('Length:',len(s1))\n",
    "while(i<250):\n",
    "    ns1=s1[i].split(' ')\n",
    "    sum1=np.zeros((300,), dtype=\"float32\")\n",
    "    w=0\n",
    "    for word in ns1:\n",
    "        for c in string.punctuation:\n",
    "                word=word.replace(c,\"\")\n",
    "        if word not in sw:\n",
    "            if(word in model.wv.vocab):\n",
    "                if w==0:\n",
    "                    sum1=np.add(sum1,model[word])\n",
    "                    w=w+1\n",
    "                else:\n",
    "                    av=sum(sum1)/300\n",
    "                    a=list(model[word])\n",
    "                    sum1=list(sum1)\n",
    "                    k=0\n",
    "                    while(k<300):\n",
    "                        if abs(a[k]-sum1[k])>=0.135: # works better at 0.135\n",
    "                            sum1[k]=a[k]+sum1[k]\n",
    "                            k=k+1\n",
    "                        else:\n",
    "                            sum1[k]=sum1[k]\n",
    "                            k=k+1\n",
    "                    sum1=np.asarray(sum1)\n",
    "    average1=np.divide(sum1,1,dtype=\"float32\")\n",
    "            \n",
    "    ns2=s2[i].split(' ')\n",
    "    sum2=np.zeros((300,),dtype=\"float32\")\n",
    "    \n",
    "    w=0\n",
    "    for word in ns2:\n",
    "        for c in string.punctuation:\n",
    "                word=word.replace(c,\"\")\n",
    "        if word not in sw:\n",
    "            if(word in model.wv.vocab):\n",
    "                if w==0:\n",
    "                    sum2=np.add(sum2,model[word])\n",
    "                    w=w+1\n",
    "                else:\n",
    "                    av=sum(sum2)/300\n",
    "                    a=list(model[word])\n",
    "                    sum2=list(sum2)\n",
    "                    k=0\n",
    "                    while(k<300):\n",
    "                        if abs(a[k]-sum2[k])>=0.135: # works better at 0.135\n",
    "                            sum2[k]=a[k]+sum2[k]\n",
    "                            k=k+1\n",
    "                        else:\n",
    "                            sum2[k]=sum2[k]\n",
    "                            k=k+1\n",
    "                    sum2=np.asarray(sum2)        \n",
    "    average2=np.divide(sum2,1,dtype=\"float32\")\n",
    "\n",
    "    sim = 1 - spatial.distance.cosine(average1,average2)\n",
    "    sim=sim*5\n",
    "    a=str(round(sim,6))+'\\n'\n",
    "    file.writelines(a)\n",
    "    #print(i,'similarity',sim)\n",
    "    i+=1\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "from scipy.stats import spearmanr\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "data1=[]\n",
    "data2=[]\n",
    "f1=open('new_vector_form.txt')\n",
    "f2=open('STS.gs.track5.en-en.txt')\n",
    "def listvalue(file):\n",
    "    value=[]\n",
    "    for line in file:\n",
    "        value.append(float(line)/5)\n",
    "    return value\n",
    "data1=listvalue(f1)\n",
    "data2=listvalue(f2)\n",
    "#print(len(data1),len(data2))\n",
    "mse = np.square(np.subtract(data2,data1)).mean()\n",
    "#print('MSE:',mse)\n",
    "ms=mean_squared_error(data2,data1)\n",
    "#print('MS:',ms)\n",
    "corr,_=pearsonr(data1,data2)\n",
    "scor,_=spearmanr(data1,data2)\n",
    "print('Pearson:',corr*100,'Spearman:',scor*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Vector based (AVF_Sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file=open('resultNP_VP12.txt','w')\n",
    "\n",
    "def partitioning(sentence):\n",
    "    sen=''\n",
    "    for word in sentence:\n",
    "        for c in string.punctuation:\n",
    "            word=word.replace(c,\"\")\n",
    "        sen+=word\n",
    "    sentence=sen\n",
    "    doc=nlp(sen)\n",
    "    nounp=[]\n",
    "    verbp=[]\n",
    "    nounp=[chunk.text for chunk in doc.noun_chunks]\n",
    "    verbp=[token.lemma_ for token in doc if token.pos_ == \"VERB\"]\n",
    "    return nounp,verbp\n",
    "\n",
    "def vectorvalue(subsent):\n",
    "    #print(subsent)\n",
    "    subsent=subsent.split(\" \")\n",
    "    sum1=np.zeros((300,), dtype=\"float32\")\n",
    "    for word in subsent:\n",
    "        if word not in sw:\n",
    "            if(word in model.wv.vocab):\n",
    "                sum1=np.add(sum1,model[word])\n",
    "    if len(subsent)>0:\n",
    "        average1=np.divide(sum1,len(subsent),dtype=\"float32\")\n",
    "    else:\n",
    "        average1=sum1\n",
    "    return average1\n",
    "\n",
    "def listtostr(phrase):\n",
    "    sent=''\n",
    "    for word in phrase:\n",
    "        sent=sent+\" \"+str(word)\n",
    "    return vectorvalue(sent)\n",
    "\n",
    "ns1=[]\n",
    "ns2=[]\n",
    "i=0\n",
    "print('Length:',len(s1))\n",
    "while(i<len(s1)):\n",
    "    ns1=s1[i]\n",
    "    #print(ns1)\n",
    "    npp,vpp=partitioning(ns1)\n",
    "    #print(npp,vpp)\n",
    "    sen1np=listtostr(npp)\n",
    "    sen1vp=listtostr(vpp)\n",
    "        \n",
    "    ns2=s2[i]\n",
    "    #print(ns2)\n",
    "    npp,vpp=partitioning(ns2)\n",
    "    #print(npp,vpp)\n",
    "    sen2np=listtostr(npp)\n",
    "    sen2vp=listtostr(vpp)\n",
    "        \n",
    "    if ((np.all(sen1np==0)) or (np.all(sen2np==0))):\n",
    "            nppsim=0\n",
    "    else:\n",
    "        nppsim= 1 - spatial.distance.cosine(sen1np,sen2np)\n",
    "    #print('nounphase:',nppsim)\n",
    "    if ((np.all(sen1vp==0)) or (np.all(sen2vp==0))):\n",
    "            vppsim=0\n",
    "    else:\n",
    "        vppsim=1 - spatial.distance.cosine(sen1vp,sen2vp)\n",
    "    #print(\"verbphase:\",vppsim)\n",
    "    #if(nppsim==0 or vppsim==0):\n",
    "       # similarity=(nppsim+vppsim)*5\n",
    "    #elif(nppsim !=0 and vppsim !=0):\n",
    "    similarity=nppsim*4+vppsim\n",
    "    #print(\"No:\",i,similarity)\n",
    "    sim=str(round(similarity,2))+'\\n'\n",
    "    file.writelines(sim)\n",
    "    i+=1\n",
    "    \n",
    "file.close()\n",
    "\n",
    "data1=[]\n",
    "data2=[]\n",
    "f1=open('resultNP_VP12.txt')\n",
    "f2=open('STS.gs.track5.en-en.txt')\n",
    "def listvalue(file):\n",
    "    value=[]\n",
    "    for line in file:\n",
    "        value.append(float(line)/5)\n",
    "    return value\n",
    "data1=listvalue(f1)\n",
    "data2=listvalue(f2)\n",
    "print(len(data1),len(data2))\n",
    "mse = np.square(np.subtract(data2,data1)).mean()\n",
    "print('MSE:',mse)\n",
    "corr,_=pearsonr(data1,data2)\n",
    "scor,_=spearmanr(data1,data2)\n",
    "print('Pearson:',corr*100,'Spearman:',scor*100)\n",
    "#P=67.51, S=70.18"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
